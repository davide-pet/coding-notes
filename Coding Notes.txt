
0 - INTRO
---------------------------

WHAT IS COMPUTED SCIENCE
        Computer Science is a lot about solving problems by taking some inputs and transforming it into an output (the solution to a problem)
        The transformation of inputs in outputs pass through algorithms. aka sets of instructions and steps
        CS inputs and outputs deals with information and data, therefore INFORMATION TECHNOLOGY  

    REPRESENTING INPUT AND OUTPUTS IN A COMPUTER
        to begin doing input-output transformation we need to store information in a standardized way
        Computers store information in 0 and 1 (binary languages), each 0 or 1 is a bit (which represent electricity stored in the machine: on and off)
        everything in a computer is ultimately represented in binary, we go up from binary to human code
        text,colors, pictures, videos, sounds are translated into binary passing through standardized mapping systems such as ASCII, UNICODE, RGB...
        series of 8 bits are called bytes
        human -> coding languages -> standardized systems to represent information -> binary -> computer

    ALGORITHMS
        are step by step instructions for solving problems, for transforming inputs into outputs
        an algorithm must be 
            - Correct (effective / providing the correct solution)
            - Efficient (optimized in terms of time and space used)
            - Well designed (in terms of code readability and usability)

        Before applied algorithms in code / programming, we can write Pseudocode
        Pseudocode is a representation of our algorithm in step by step english

        Algorithms are composed by some major building blocks
            - Functions (basically actions that solve smaller problem and that we ask the machine to do)
            - Conditionals (if conditions, like forks in a road)
            - Loops (repeating actions)
            - Boolean expression (comparisons of two things that results in a yes or no / true or false / 0 or 1)
            - arguments (input data)
            - variables (elements that stores changeable  information, like changeable input or outputs)
            - data structures (structures of data that help solve problems by more easily accessing and manipulating information)

BUILDING BLOCKS 

    Beginner:
         - variables
         - Conditions and chained conditionals
         - operators
         - If / else
         - Loops
         - Basic Data structures (list, dicts, strings, sets, tuples, )
         - Functions
         - Mutable vs Immutable
         - Common Methods
         - File Input and output

    intermediate
        - Object Oriented Programming
        - Data Structures and Algorithms
        - Comprehensions
        - lambda, Map and Filter 
        - PIP, Modules and Packages
        - Args and kwargs
        - Async io 

    Expert
        - Decorators
        - Generators 
        - Context Managers 
        - Concurrency vs Parallelism (locks and multithreading)
        - UNit Testing 
        - Cython

    Master
        - Pthon working under the hood, compilers and interpreters
        - how computers works
        - how internet works
        - system design
        - Machine Learning and AI
        - Data Science
        - Software and web development
        - Apis 



A - PHYTON
---------------------------

0) RETRIEVE INFO

    print(dir('modulename'))                -> module name eg. math / print all the functions that are available in that module
                                                can also be written:
                                                    for method in dir(math):
                                                        print(method)

    help(function/class/modules/keywords)   -> retrieve the documentation of that function

    ?                                       -> in jupyter

1) VARIABLES , OPERATORS AND EXCEPTIONS
    
    a. Variable assignement with '=' 
        best practice 1. letters: eg. A, b, X , Y, Z ecc
        best practice 2. Meaningful variables: hours, pay, rates...
            CAPs matters
            avoid confusion

    b. types of stuff: 
        integer, floating, string, Boolean true/false

    c. operators:  
        +
        -
        *
        /
        ** (power)
        % (remainder)

    d. operator precedences:
        parenthesys
        power
        multiplication and divisions
        addition
        left to right
        not
        and
        or
    
    e. comparison operations
        <   less than
        <=  less or equal than
        ==  equal
        >=  more or equal than
        >   more than
        !=  not equal
        is, is not (both in type and value)
        in, not in
        and, or, not

    De Morgan's Laws
        not (x and y) == not x or not Y
        not (x or y) == not x and not y 

    f. assignment operators
        =
        +=  -> eg. x += 1 -> x = x + 1
        -=
        *=
        /=
        %=
        **=

2) RESERVED WORDS and FUNCTIONS

    Functions -> creates a new element
    Methods -> do stuff in place and modify the existing element

    a. Reserved Words:
        false, none, true           -> types boolean and type None 
        and, in, is, not, or        -> operators for comparison
        if, elif, else              -> conditional words
        for, while,                 -> loop words
        try, except, finally        -> try-exeption blocks words
        break                       -> to interrupt an operations and exit a loop/conditionals immediately
        return                      -> to return the result of an operation
        continue                    -> continue to the next iteration of a loop
        pass                        -> a null statement, not ignored but nothing happens
        raise                       -> to raise an exception
        class                       -> to open a class in object-oriented programming 
        def                         -> to open a new function
        as                          -> to create aliases
        assert                      -> for debugging
        del                         -> delete stuff
        from                        -> for modules 
        global                      -> to assess global variables inside a function. Bad Practice to use.
        import                      -> to import modules like .math .random ecc
        lambda                      -> a function made by lambda argument : expression
        nonlocal
        with
        yield 

    b. Built-in functions:        
        
        Print()                     -> print stuff on the terminal 
            r before the string set it as a raw string without escape chars
        input()                     -> ask users for an input
        
        int()                       -> transform stuff into integers
        float()                     -> transform stuff into floats
        str()                       -> transform stuff into strings
        type()                      -> get the type of an object
        ord() e chr()               -> get the ordinary number of a string and viceversa
        round()                     -> round a number

        min()                       -> get minimum value 
        max()                       -> get maximum value
        abs()                       -> get the absolute value 
        pow(x,y)                    -> return value of x to the powwer of y     
        len()                       -> get the lenght of an object
        
        dict()
        tuple()
        set()
        list()      
        enumerate()                 -> Takes a collection (e.g. a tuple) and returns it as an enumerate object

        isinstance(var, type)       -> tells you if an object is of a certain data type and return a boolean                
        range()                     -> mostly for creating for loop ranges
        reversed()                  -> returns a reversed iterator
        split()                     -> split stuff 
        sorted()                    -> sort stuff in ascending order | reverse=True for descending order 
                                    sorted() with a key function def / sorted(object, key = function_name)

    c. More useful stuff
        
        slicing []                  -> with [], reverse slice with [::-1]
                                    position starting from 0, second place means "up to but not including"

        unpacking collections       -> fruits = ['apple', 'banana', 'orange']
                                    x, y, z = fruits
                                            * is the splat parameter and can also be used to separated items in a collection into individual elements
                                            * unpack a list or similar
                                            ** unpack a dictionary

        lambda function             -> lambda argument : expression
                                    A lambda function can take any number of arguments, but can only have one expression.
                                    eg x = lambda a : a + 10
                                       print(x(5))
                                    -> lambda can be used as an argument
                                        for example as the key to sort stuff

        Map(function, iterable obj) -> Map function is mostly used with lambda
                                    -> we use map to iterate in a list (just like a for loop)
                                    -> map() applicates the function and returns its NEW VALUES
                                    -> it just return the values, not the list of values

        Filter(function, iter. obj) -> Also Filter is used with lambda
                                    -> filter return values from a list only if they respect a condition, just like a for+if
                                    -> filter() applicates the function to filter but returns the ORIGINAL FILTERED VALUES
                                    -> it just returns the values, not the list of values

    d. Def() functions:
        Functions we define ourselves - define a function and store it for invoking it later
       
        def functionname()          -> create a function
        functionname()              -> invoke a function
        def yourfunctionname()      -> no parameters function with
        def yourfunctionname(value) -> single parameter function with
        def addtwo(a, b)            -> multiple parameters functions with
            added = a + b

        *args -> if you don't know how many arguments will be passed add an * to the function arguments
            def functionname(*args)   / a tuple of arguments will be passed when invoked
            ....functionname('carl', 'jack', 'simon')

        *kwargs -> if you don't know how many keyword arguments will be passed add a ** to the function arguments
            def functionname(**kwargs)  / a dictionary of arguments will be passed when invoked
            ....functionname(name: 'jack', surname: 'simonson')

        * is the splat parameter and can also be used to separated items in a collection into individual elements
            * unpack a list or similar
            ** unpack a dictionary

3) MODULES /LIBRARIES and PACKAGES

    ---- BUILT-IN AND MOST USED MODULES ----

        1. datetime module

            to work with date formats
            2022-01-11 15:17:33.589263 The date contains year, month, day, hour, minute, second, and microsecond.

            import datetime             

            datetime.datetime.now()                         -> return current time
            datetime.datetime(y, m, d)                      -> to create a date
            datetime.strftime(format)                       -> trasform a date into a string
            datetime.fromtimestamp()                        -> trasform a timestamp into a date

        2. math module
            import math
            math.sqrt()                 -> return the square root
            math.ceil()                 -> round a number upword to integer
            math.floor()                -> round a number downward to integer
            math.pi                     -> return the value of pi 3.14 constant

        3. statistic module
            import statistics
            statistic.means()
            statistic.median()
            statistic.variance()
            ....

        4. Random module
            import random
            .random.random()            -> generate a nr between 0 and 1
            .random.randrange()         -> random nr in a range
            .random.choice()            -> random element from a list
            .random.choices()           -> more random elements from a string
            ...

        5. regex module: 
            import re

        6. Os module
            
            https://www.youtube.com/watch?v=tJxcKyFMTGo
            
            to interact with the operating system. mainly directories and files

            import os

                os.getcwd()                                 -> return the current working directory
                os.chdir('path')                            -> navigate to the 'path' and change the current working directory
                os.listdir()                                -> list all file and folders in the current directory(by default), or you can add a path in the parenthesis
                os.stat('file name')                        -> retrieve information about a file eg. last edit, size ecc... / can also add specfic info at the end / eg. os.stat('filename').st_size
                os.walk('path')                             -> a generator (iterable) to see all the directory tree deep down the paramenter path / very useful to search things
                                                            -> it returns a tuples with 3 info: directory path, all directory under that path, all files under that path   
                                                            for dirpath, dirnames, filename in os.walk('path'):
                                                                print('current path:', dirpath)
                                                                print('directories:', dirname)
                                                                print('files:', filenames)
                                                                print()   

                os.mkdir('folder orfilename')               -> used to create folder, just one level
                os.mkdirs('foldername/foldername/...)       -> BEST CHOICE, used to create a folder, can be multiple levels deep 
                os.rmdir('folder orfilename')               -> BEST CHOICE, used to remove a folder, directly just 1 level
                os.removedirs('foldername/foldername/...)   -> used to remove a folder, multiple levels up
                os.rename('oldname', 'newname')             -> used to rename a file or folder

                os.path -> is a powerful package under os that let's you manipulate path in useful ways

                    os.path.join(path1, filename)           -> let's you combine 2 paths or path + file name to get to a specific file to open
                                                            -> file_path = os.path.join(path1, path2)
                                                            -> with open(file_path, 'r') ecc...

                    os.path.basename(path)                  -> returns the full name of the path
                    os.path.dirname(path)                   -> returns the directory name of that path
                    os.path.split(path)                     -> returns the directory nme and the filename
                    os.path.exists(path)                    -> check if a path exists
                    os.path.isdir(path)                     -> check if the path is a directory
                    os.path.isfile(path)                    -> check if the path is a file
                    os.path.splitext(path)                  -> split the filename and the extension

        7. pathlib module
            
            to interact with the operating system. mainly directories and files. 
            newer modules updating the os one.

            https://realpython.com/python-pathlib/

            import pathlib
            from pathlib import Path                        -> since you will mainly use the Path class 

            accessing / creating paths 
                Path.cwd()                                  -> access / return the current working directory
                Path(r'raw path')                           -> access / return a specific path
                Path.home()                                 -> access / return the home path
                path_object.joinpath('path/file to join')   -> joing paths to make a single one / eg. Path.home().joinpath('stuff', 'stuff2','filename')
                path_object / path / path / path            -> same as above, its a shortcut for joinpath() / eg. Path.home() / 'stuff' / 'stuff' / 'filename'
                Path(__file__)                              -> access / return the specific current working filename path
            
            reading and writing files
                
                1. as a function
                path = create a path here...                
                with open(path, 'r') as filename: ....

                2. as an object
                path = create a path here....
                with path.open('r') as filename: ....

            writing / creating / doing stuff

                first you joinpath with the name of the file or the folder

                then
                path_object.touch(exist_ok=True)            -> creates a file / if it exists, it simply overwrites it / if false and it exists, return error
                path_object.mkdir(exist_ok=True)            -> as above, but creates a folder   

                .iterdir()                                  -> iterate through a directory. its an iterator you can use with for                                 

            path components 
                .name                                       -> the file name without any directory
                .parent                                     -> the directory containing the file, or the parent directory if path is a directory
                .parents                                    -> return the parents of the path in tuple accessible by indexing []
                .stem                                       -> the file name without the suffix
                .suffix                                     -> the file extension
                .anchor                                     -> the part of the path before the directories
                .parts                                      -> break the path into its different parts and adds them to a tuple / therefore they are accessible by indexing []
                .isfile()                                   -> true - false if it is a file
                .isdir()                                    -> t- f if it is a directory

        9. pytest

        10. urllib / request

            https://realpython.com/python-requests/
            https://www.youtube.com/watch?v=tb8gHvYlCFs

            pip install request
            import request

            request.get('url')                           -> make a request to retrieve data from a server/source
            response = request.get('url')                -> to store the request response into a variable
            response = request.get('url', params={})     -> customize the get requests with parameters
            response = request.get('url', headers={})    -> customize the get requests headers with headers parameters

            requests.post('url', data={'key':'value'})   -> to send data. [ can also pass data as json = {} ]
            requests.put('url', data={'key':'value'})    -> also to send data, but repetition has no effect
            requests.delete('url')                       -> to delete the specified source (file, record etc..)
            requests.head('url')                         -> to send just a head request (without the body)
            requests.patch('url', data={'key':'value'})  -> not used a lot. to modify capabilities and change the source
            requests.options('url')                      -> to get the supported options / http methods for the target resource 
                                                            (used before sending the actual request)

            http request information with get
                response.status_code                    -> return the status code 200, 300, 400, 500...
                response.raise_for_status()             -> raise an http error if one occured     
                response.content                        -> see the content in bytes
                response.text                           -> see the content in text --> response is a serialized json
                    response.encoding = 'utf-8'         -> optional if you want to set a specific character encoding method

                whene you get the response.text in a serialized json you can use the followings to make it a dict:
                    json.loads()                        -> basically the original method with import json
                    response.json()                     -> a shortcut method of the request library

                response.headers                        -> to access the headers (returned as a dict like object with k-v pairs)
                response.headers['Content-Type]         -> being a dict, you can access all elements of headers


            basic Authentication scheme
                request.get('url', auth = ('username', 'password'))
                for privacy you can also input the username and password in the terminal using
                    from getpass import getpass, getuser
                    request.get('url', auth = (getuser(), getpass()))

            explicit authentication scheme
                from requests.auth import HTTPBasicAuth
                from getpass import getpass
                requests.get('url', auth=HTTPBasicAuth('username', getpass())

            Timeouts and max retries
                requests.get('url', timeout=1)          -> the timeout parameter represents in seconds (int or float)
                                                        how long should we wait the response before moving once
                                                        useful also to avoid waiting indefinitely.

                request.get('url', HTTPAdapter(max_retries=3))  -> nr of attempts to try before raising a connection error  
                                                                    usually one within sessions

    ---- PYPI MODULES ----
    
        a. PyPI and pip install
            PyPI -> It's a directory online with a lot of packages created by people
            No need to download the file, 
            using the keyword "pip install" in the command line, and the importing them, you can use them

        b requests module
            import requests
            used mainly to import api json files (in conjunction with import json)

        c. pytest package
            to implement unit testing

    ---- CUSTOM MODULES AND PACKAGES ----

        a. what is a module?
            it is the same as a code library
            a file .py containing a set of functions that you want to include and import in your application
            there are built-in modules and custom modules.    

        b. basic instructions
            create a module             -> just create a file .py 
            import my_module            -> to import a module
            from - import               -> import only a specific stuff from the module
            my_module.                  -> to recall a module function
            dir()                       -> shows you all the functions of a module

        c. What is a PACKAGES
            simply a directory containing multiple python modules
            a package needs an __init__.py file (module)
            the __init__.py will initialize the package

        d. basic instructions

            direct imports
                import packagename.modulename  -> to import a module in a package
                import packagename             -> improt the entire package

            import through __init__
                the main file import the package: import packagename
                while the __init__ module import all the modules with import packagename.modulename

            nested packages 
                just import with a . from higher in hierarchy to lower    -> import pack1.pack2. pack3...

        e. if__name__ = "__main__":
            use it to separate stuff that you only want to run in a module when accessing it as main but not as an imported module
            -> if you run the module as main, this stuff will run
            -> if you run this module as imported, this stuff will not run

4) IF

    a. if                           -> one way decision
    b. if else                      -> 2 way
    c. if elif else                 -> multi way

5) FOR and WHILE

    a. While loops

    b. For in + list of stuff in []

        For - range(start, stop, step)

        #iterate by index
            lst = [1, 4, x, k, 6, True]
            for i in range(len(lst)):
                print(i, lst[i])
        
        #iterate by element
            lst = [1, 4, x, k, 6, True]
                for element in lst:
                    i = lst.index(element)
                    print(element, i)

        #terate by both with enumerate
            lst = [1, 4, x, k, 6, True]
            for i,element in enumerate(lst):
                print(i, element)

    c. else:
         with for and while 
         With the else statement we can run a block of code once when the loop is finished
         if the loop breaks with 'break', the else is not executed because the loop is not finished
  
6) STRINGS

    
    Main methods:
        .find()  / .index()         -> to find position
        .split()                    -> default spilt is based on spaces | ou can specify with split (" ; ")
        .count()                    -> return nr of time a specifie value occurs in the string 
        .upper()                    -> tutto maiuscolo
        .lower()                    -> tutto minuscolo
        .capitalize()               -> first character in upper case and the rest in lower case
        .casefold()                 -> converts into lower case
        .replace(replaced,new_word) -> replace stuff
        .strip() .lstrip() .rstrip()-> to remove black spaces
        .startswith()
        .endswith()
        .center()                   -> return a centered string
        .encode()
        .isalnum()
        .isalpha()
        .isdecimal()
        .isdigit()
        .isidentifier()
        .isnumeric()

7) LISTS []
   
    ordered, changebale and allow duplicate values
    indexed starting from 0

    a. basic functions
        identified with []          -> example: friends = ["james", "janeth", "jackson"]
        lst = list() / []           -> open a list and convert into a list

    b. Main methods:   
        .append()                   -> add an element at the end of the list. 
        .extend                     -> extend the current list by adding elements of aother list or pbject
        .insert(position, value)    -> insert an item at the specified position
        .pop(index)                 -> remove the item at the specified index or the last element by default  
        .remove(item)               -> remove the first occurrence of an element
        .clear()                    -> clear the list and makes it empty
        .count()                    -> count how many times an items appears in the list
        .index()                    -> find the index of an element 
        .reverse()                  -> reverse the order of a list
        .sort()                     -> sort ascending order 
        .copy()                     -> copy a new list but creating a different object from the original list
        .count(value)                    -> count the nr of element in the list with the specified value

    c. list comprehension -> to create a new list quickly
        
        newlist = [expression 'for' item 'in' iterable 'if condition == True']

            fruits = ["apple", "banana", "cherry", "kiwi", "mango"]
            newlist = [x for x in fruits if "a" in x]
     
8) TUPLES ()
    
    ordered, unchangeable, allows duplicate members

    a. basic functions
        identified with ()           -> eg. y = ("primo", "secondo", "terzo") or x = (1, 2, 3)   
                                        main difference is taples are immutable, thus they'r more efficient 

        tpl = tuple() or simply created with variable = (item, item, item..)
                         a tuple with 1 element is made with variable = (item1, )
        
    B. Main methods
        how to change a tuple        -> change it into a list(), modify it, remake it into a tuple()
        .count()
        .index()

9) SETS {}

    unordered, unchangeable, unique no-duplicte members

    a. basic functions  
        structured with {}             -> e.g set = {1, 2, 3, 4, 6, 8} | unordered collection of unique elements
        set = set()                    -> open a set or transform into a set
        
    b. main methods
        .add() / .update()             -> add stuff. add with single item. update with other sets      
        .remove()                      -> remove stuff from the set
        .discard()                     -> remove stuff
        .clear()                       -> clear stuff / remove everything
        .copy()

        
        set1.union(set2) or        -> combine 2 sets together   
        set1.update(set2)          -> add all elements of a set to another set
        set1.intersection(set2)    -> find intersection of 2 sets
        set1.difference(set2)      -> what is in set1 but not in set2
        set1.symm_difference(set2) -> find symmetric differences 
        set1.issubset(set2)        -> superset: viceversa
        set1.issuperset(set2)      -> superset: contains all the elements of another sets and more

    c. you can only add immutable object in a set
        cannot add a list [] and dict {} in a set
        but you can add tuples ()

10) DICTIONARIES {key:value}

    unordered, changeable, unique no-duplicate members
    assignment key and values in pairs

    a. basic functions
        identified with {}          -> eg. dict = {'cat':2, 'dog':3, 'mouse':1}
        dict = dict() / {}          -> open a dictionary and convert into a dictionary
        
        x["key"] = "value"          -> add values with
        del["key"]                  -> delete stuff

    b. Main methods:
        .keys()                     -> access keys
        .values()                   -> access values
        .item()                     -> access keys and values in tuples
        .get()                      -> get the value of stuff
        .update()
        .pop(key)
        .clear()
        .copy()
        

    c. loop in a dictionary with

        # by items
        for keys, values in dict.items():
            print (keys, values)

        # or by keys
        for keys in x:
            values = x[keys]
            print(keys, values)
    
        .get() -> dict[key] = dict.get(key, 0) + 1
            add keys from scratch or add values to existing keys with get() (you don't know if the key exist)
            # se non c'è aggiungila con valore 0+1 iniziale, se c'è fai +1 al suo valore

11) TRY, EXCEPT, FINALLY

    try: lets you test a block for errors
    except: lets you handle errors
    finally: lets you execute regardless of the try-except block        

    you can also add an 'else' to the except block.
        else activate if the try worked and there are no errors  

    handling specific errors
        Except errorname as x

    handle multiple exceptions with
        add multiple except line

    finally: block
        whatever happes in the try-except blocl, the finally: block execute anyway

    raise an error with the following to create custom errors
        raise errorname("this is an error")  / for specific errors
        rais exception("this is an error")   / for general errors

12) CLASSES AND OBJECTS

    a. Definitions

        objects / instance : 
            is an instance of a class. the existences of an object of a specific class.
            the term instantiation is defined as the creation of an object based on a class
            basically everything is an object. eg. (x = 2) is an object made by x that assumes the value 1 of class int.
            object oriented programming is a coding way based on interactions between objects

        data types / classes: 
            it's the bluepring that specify what a certain object can do and its properties
            eg. int is a class, string is a class, function is a class..
            the type of an object is the class used to instantiate it.

        methods
            are functions that you call on specific instances of a class and that live insides of a specific class.
            they are not general functions. eg. string methods are good for strings, not integers.


    b. Creating a Class

        creating a class with
            class ClassName:
                convention is to name classes with first letter in uppercase. Person or BigCars or TikiTaka

        assign class to an object with
            x = ClassName()

        initialize a method and assign properties to a new class with
            def __init__(self):


    c. Attributes

        assign attributes to specific instances of the class with
        
        class ClassName:
            def __init_(self, attr1name, attr2name):
                self.attr1name = attr1name
                self.attr2name = attr2name

            x = ClassName("attr1value, attr2value)


    d. Methods

        create a method by defining a function inside the class with self as a parameter

            def function_name(self)
                --> here what this method does

        invoke the function with
            x.function_name()


    e. Private Attributes and Properties

        step 1 : create a private attribute with
             _privateattributename

        create a property with getter and setters
            getter return the value of an attribute
            setters validate the value of an attribute

            @property
            def propertyname(self)
                write here the return of the property / this is the getter parenthesys
            +
            @propertyname.setter
            def propertyname(self, _privateattributename)
                write here the setter part


    f. Class Methods and Class Attributes
    
        class attributes
            differently from classic instance attributes these are attributes related to the entire class
            they are defined at the beginning of the class before any instantiation
            they are invoked with:
                ClassNam.ClassAttributeName

        Class method
            used to access and define functions that operate on class attributes
            class attributes inside class methods get invoked with cls.
                @classmethod
                def update_number_of_cars(cls, cars):
                    cls.number_of_cars = cars

    g. static methods with
            Static methods are denoted with the @staticmethod decorator and take no mandatory parameters.
            You can think of them as utillity functions that are defined inside of a class. 
            They have no access to instance attributes, class attributes or methods.
            
            @staticmethod
            def static_method_name(parameter) --->> no self or cls
                return ....

    e. Inheritance with

        create a child / sub class with
            class child_class_name(parent_class_name):

        override a method in a child class with
        = it lets you access methods from the parent class    
            super().parent_class_method_name()


        override a constructor with
            def __init__(self, ...parent_parameter.., new_child_parameters):
                super().__init__(parent_paramenters)
                self.child_paramenter = child_parameters


    f. abstract classes and abstract Methods

        An abstract class is designed to act as the base class in an inheritance hierarchy
        and is not designed to be instantiated. Abstract classes usually contain abstract methods that 
        are meant to be implemented by classes that inherit from them.

        All method types are allowed in an abstract class. Abstract classes usually contain some concrete methods 
        (ones that provide an implementation) while also defining abstract methods that must be implemented by child 
        classes that inherits from it.

        An abstract class should not be instantiated and is meant to function as a base class that provides already 
        implemented behavior that can be extended by any subclasses to create a full implementation.
         Abstract classes often contain abstract methods that are meant to be implemented by any
          subclasses to create a full implementation of the abstract class.

        create it like a normal class but add Abstract to the name
            class AbstractGame:

    g. Interfaces

        An interface is an abstract data type that is meant to be implemented by a class. 
        Any class that implements (in Python, inherits) an interface must implement all of the methods
        defined in the interface. This makes the interface act like a blueprint for the class.
        Interfaces provide no concrete implementations and do not reduce the amount of code needed
        for classes implementing them
    
    h. Dunder methods

        addition with:
            def __add__(self, ...)

        subtaction with
            def __sub__(...)

        multiply with
            def __mol__(...)

        division with:
            def __truediv__(...) / regular division
            def __floordiv__(...) / integer division
        
        len with
            def __len__(...)

        check equality with:
            equality --> def __eq__(....)
            inequality --> def __ne__(...)

        greater than
            def __gt__(...)

        greater or equal 
            def __ge__

        less than
            def __lt__(...)

        less or equal 
            def __len__

        string function
            def __str__(....)
            def __repr__(....)
        
13) FILE HANDLING

    1. file handling functions and methods
        
        classic way (open and close)       
            open(filename, mode)                                 -> example handle = open(prova.txt, r)
                mode r is for reading / if file is in the same folder ok, otherwise specify the path
                mode w is for writing (it overwrites) create the file if it doeas not exist
                mode x is for creating a file
                mode a is for appending stuff to another file, creates the file if it does not exist
                mode r+ is for reading + writing -> mostly used with .seek()
                mode wb is for writing bytes
                Files are to be considered as a sequence of strings on each line.

        with context manager (automatic closing)
            with open('filename', mode) as XXX
                you can open file with the sintax:               -> the with automatically close the file after the operations indented are done
                    with filename as variable name
                        do stuff with indentation
                

        .read()                                              -> read the content of the file
        .readline()                                          -> gives you a list of the lines in the file
        .readline()[i]                                       -> can be used to return specific lines
        .close()                                             -> close the file
        .write()                                             -> write stuff
        .seek(position)                                      -> change the cursor in the file in a specified position
            position 0 = beginning of the file
            position 1 = current position in the file
            position 2 = end of the file
    
        
    2. import csv an txt 

        csv.reader() -> split the csv automatically into lists
            + use delimiter = 'xxx' to set a specific parsing delimiter (instead of default comma)

        csv.DictReader() -> split the csv automatically into lists
        
        csv.DictWriter(filename, fieldnames=) -> takes in a dictionary to write it in a csv format
        writerow() -> write a line of csv (usually with csv.DictWriter
        )

    3. import databases

        import sqlite3 

        conn = sqlite3.connect('dbname')                    -> creates a connection to the db and store it into an object
        cur = conn.cursor()                                 -> create a cursor. A cursor allows us to execute sql queries
        cur.execute(wrote query here)                       -> execute query
        results = cur.fetchall()                            -> fetch all query results in a variable

        remember to close the cursor and connection
            cur.close()
            conn.close()


    4. Import OS

        import os 

            os.remove()                                      -> to delete a file 
            os.rmdir                                         -> to delete a folder

14) DECORATORS, ITERATORS, GENERATORS

    a. DECORATORS
        - they are called with @
        - just liche @staticmethid
        - there are built-in or custom DECORATORS

        to create a decorator build a nested function
            def decorator(func):
                def wrapper(*args, **kwargs):
                    result = funct(*args, **kwargs)
                    return result
                return wrapper

        to call a decorator
            @decorator
            def foo():
                print('foo')

    b. ITERATORS
        Iter()          -> function that transform an object into an iterator
        .__iter__()     -> method that transform an object into an iterator
        next()          -> move through the iterator

    c GENERATORS
        special kind of iterators used the majority of time (more used than iterators)
        very space efficient. used to generate infinite lists but without storing them and then accessing specific stuffs generated
        made by defining a function + kw yield

        def generator()
            + yield

15) THREADING

    a. threads
        Import threading                                -> to import the module for creating threads
        thread1 = threading.Thread(target = function)   -> to create a threading
        thread1.start()                                 -> to start a thread
        thread1.join()                                  -> waiting for the thread to finish before moving to the next one
        threading.active_count()                        -> tells you how many threads are ctive in the program, including the main one.

        remember to always join the threads at the end of a program

    b. locks
        from threading import Lock, Thread              -> to also import locks
        mutex = lock()                                  -> creates a lock             
        mutex.acquire()                                 -> acquires / assign a lock
        mutex.release()                                 -> release a lock after it has been used

        deadlock = no threads are able to run           -> be careful, there's no error message

    c. Delays
        from time import sleep                 
        sleeep(timetoslee)                              -> slow down and sleep a function for a specific amount of time                                  

16) ASYNCHRONOUS PROGRAMMING

    import asyncio                      -> import the module 
    
    async def functionname()            -> create a coroutine
    asyncio.run()                       -> run a coroutine. it call an async function
    await coroutine()                   -> literary awaits for a coroutine to run / it must be inside functions. basically to call other coroutines inside another onw
    
    asyncio.sleep()                     -> it pauses and sleep the coroutine (use this one from the async module with coroutines)
    asyncio.create_task(function())     -> it creates a task = an operation to run as soon as possible in the future - that must be awaited

    await asyncio.gather(func(), func())-> run multiple tasks concurrently

17) UNIT TESTING

    tool nr 1: create a separate test file to test your program
    tool nr 2: use keyword "assert"
    tool nr 3: use the PyPI "pytest" library
        pip install pytest
        create a dedicate py file
        test using comm line "pytest namefile.py" instead of "python namefile.py"

18) CRUD - CREATE, READ, UPDATE, DELETE

    Lists 

        create
            listname = [1,2,3]
            listname = []
            listname = list()

        read
            print(listname)
            listname[0] --> 1
            listname[-1] --> 3
            len(listname) --> 3

        update
            listname[0] = 2 --> [2,2,3]
            listname.append(4) --> [2,2,3,4]
            listname.insert(1, 4) --> [2,4,2,3,4]

        delete 
            del listname[0] --> [4,2,3,4]
            listname.pop()
            listname.pop(index)

    dictionaries

        create
            astronaut = {
                'name': 'elon musk'
                'suit_size': 'large'
                'allergies': 'peanuts'
                }
            astronauts = {}
            astronauts = dict()

        read
            print(astronauts)
            astronauts['name'] --> elon musk
            astronauts.keys()
            astronauts.values()

        update
            astronauts['allergies'] = mango
            astronauts['space ship'] = 'galactic1'

        delete
            del astronauts['space ship']
            astronauts.pop(key)



B - OTHER LANGUAGES
---------------------------

1) BASH (to learn better)

    pwd                             -> print working directory
    cd foldername                   -> change directory
    cd .                            -> 1 dot is the current directory
    cd ..                           -> each additinal dot means the directory 1 level above
    cd /foldername                  -> go to the specified directory in a path eg (cd ../directoryname = go to parent direct and then to directoryname)
    ls                              -> shows all file and subdirectories inside the current directory in alphabetical order
    clear                           -> clear the screen
    
    tab shortcut                    -> autocomplete the directoryname
    up arrow                        -> return the previous command you used
    down arrow                      -> return the following command you used
        
    touch filename.type             -> create a file
    cat > filename.type             -> create a file and allows you to type
        control+c                       -> when done typing, close with ^C
        control+d                       -> save the file
    cat file1 >> file2              -> concatenate the 2 files by appending file1 to file2
    
    mkdir directoryname             -> creates a directory
    mkdir directory/subdirecname    -> create a subdirectory inside an existing directory
    
    rm nameofstuff                  -> remove files 
    rm -r directoryname/            -> remve directory -r = recurseverly delete every file inside of a directory and the directory
    mv                              -> move files. use it: "mv filename.type newdirectoryname "
    cp                              -> copy files and directories. "cp filename directorywhereyouwanttopasteit"
    nano                            -> open a text editor
        control+X                       -> to save and exit the nano editor

    pyhton3                         -> opens the python Language
                                    with nano you can create .py files and write immediate scripts

    python3 + name of a .py file    -> it will run the python file

    find nameoffile                 -> search a file in the current directory
    find . -name filename           -> search a file in the current directory and its subdirectories
    find . -regex " "               -> you can search for stuff with regex characters in the shell enviroment

    head                            -> gives you the first 10 lines of a file 
    head -n 2                       -> return the first n lines of a file. in this case 2                            
    tail                            -> same as head but tail 
    tail -n 2                       -> idem 

    grep "char to search" filename  -> looks for specific stuff in a file
                                    -> it returns the lines that contains that "char to search"

    ls | grep "txt"                 -> you can use grep to filter files that contains specific char. for example all .txt or all files that have an "a" in the name

2) REGEX (to learn better)

    Import re

    Functions
        re.search(pattern, string, flags = 0)               -> search if something is present, return boolean
        re.match(pattern, string, flags = 0)                -> check if something is equal to, return boolean
        re.fullmatch(pattern, string, flags = 0)            -> check if something is equal to, return boolean
        re.findall(pattern, string, flags = 0)              -> extract all the matching strings
        re.sub(pattern, repl, string, count=0, flags=0)     -> substitute a pattern with a new pattern
        re.split(pattern, string, maxsplit=0, flags=0)      -> split stuff based on a pattern 

    :=  the walrus sign. means both assignement and function exection. eg if matches := operation, ---- (if true both execute and assign the value to match)

    flags
        re.IGNORECASE
        re.MULTILINE
        re.DOTALL

    syntax
        ^       matches the BEGINNING of a string 
        $       matches the END of a string 

        .       ANY Character except a newline
        *       0 or more repetitions of the char at its left
        +       1 or more repetitions of the char at its left
        ?       0 or 1 repetitions of the char at its left (boolean)
        \       escape char 

        ...     more . characters means any n character = .. 2 char = ... 3 char and so on 
        {m}     m repetitions
        {m,n}   m - n repetitions

        []      a set of characters (included set of characters). eg [aeiou], can also be a range [a-z0-9_ ]
        [^]     compelmenting set of char (excluded set of characters). eg [^@]

        A|B     either A or B
        (...)   a capturing group
        (?:...) non-capturing version of a group

        some shortcuts
        
        \d      matches a decimal digit
        \D      matches everything exept a decimal digit
        
        \s      mathches a whitespace characters
        \S      matches everything except a whitespace character
        
        \w      matches a word character … as well as numbers and the underscore
        \W      matches everything except a word character


    
    
    
    
    
    
    \        is an escape character that gives literal meaning to stuff (eg: . means any char - \. means the . charact)
    \s       matches WHITESPACE
    \S       matches any NON-WHITESPACE Character
    *        means zero or more occurrencies of the preciding text (eg: a*)
    *?       REPEATS a character zero or more times (non-greedy)
    +        REPEATS a character one or more times
    +?       REPEATS a character one or more times (non-greedy)
    [aeiou]  matches any of the character in the LISTED store
    [^XYZ]   matches a singole character NOT IN THE LISTED store
    [a-z0-9] the set of characters can include a range
    (        indicates where string EXTRACTION IS TO start
    ]        indicates where string extraction is to end

3) SQL (to learn better)

    database: SQLlite
    Language: SQL

    CREATE
    INSERT INTO - VALUES
    DELETE FROM - WHERE 
    DROP
    UPDATE SET - WHERE
    SELECT*FROM - WHERE
    ORDER BY - DESCENDING
    JOIN - ON
    CONNECTOR TABLES for many to many relationships

    Rule nr1 : don't replicate string data more than 1 times
    Rule nr2 : use keys and add a special column to create relationships. the ID column
        Primary key -> primary integer key
        Logical key -> the name or what outside world uses for lookup
        Foreign key -> integer key pointing to the primary key in the other table

4) GIT - GITHUB (to learn better)

    git = local repositories
        
        git init                        -> initialize a local repository
        git status                      -> get repository status
        git add stuffname               -> add a file to the staging area 
        git add .                       -> add everything in the current directory to the staging area
        git commit -m "add filename"    -> commit a file to a local repository

        git branch                      -> returns the branch you are in
        git checkout -b branchname      -> checkout means switch to a new branch (-b) called "branchname"
        git cheackout master            -> switch to the master branchgit

        git merge                       -> merge branches 
                                        -> merge the selected branch in into the branch you are in
                                        -> so be in the master branch if you want to merge stuff into it

        git -D                          -> delete stuff

        git rebase                      -> perform a rebase


    github = remote repositories  

        git remote add origin +githuburl                      
        git pull
        git push origin
        git clone 

5) JSON

    Json Syntax
        - Data is in key-value pairs
        - Data is separated by commas
        - Curly braces hold objects 
        - Square brackets hold arrays

        - keys must be strings. 
        - value can be strings, numbers, json objects, arrays, boolean, none
        - with strings data double quotes are required ""

    Coding and decoding json

        - https://docs.python.org/3/library/json.html#basic-usage
        - https://www.youtube.com/watch?v=9N6a-VLBa2I
        
        - import json

        - json.dump() or json.dumps()
            dump() --> write data into a json file
            dumps() --> write data into a json string
            dump(the object to be serialized, the file to which the bytes will be written)

        - json.dump(data, indent = 4, sort_keys = True)
            sort dict items by key
            change whitespace for indentation

        
        - json.load() or json.loads()
            load() --> load a file. so you have to complete with file i/o functions
            loads() --> load a string from a file
            deserialize json data. turn json encoded data into Python objects
        
    Json encoding and decoding process

        Import the json package.
        Read the data with load() or loads().
        Process the data.
        Write the altered data with dump() or dumps().

    Working with custom objects (classes) that json doesn't understand
        transform the object into a dict with a lambda x : x.__dict___



C - SOFTWARE DESIGN
---------------------------

1) DATA STRUCTURE AND ALGORITHMS

    a. Complexity Analysis
        
        done to evalueate if a solution is better than another one.
        the better the space-time complexity is, the better the solution
        
        evaluation based on 2 scientific concepts
            - time Complexity: how much time to run an algorithm
            - space complexity: how much space it uses in memory

    b. Big O Notation -> express the space-time complexity of an algorithm
            = the measure of an algorith speed / runtime as the size of the input increases

            O(1)         -> Constant time complexity: as the size of input increase, the speed ot the algo remains constant
            O(log(N))    -> Logaritmic time complexity: 
            O(N)         -> Linear time complexity: as the size of the input increases, the speed increases linearly
            O(Nlog(N))   -> Log-linear time complexity: 
            O(N2)        -> Exponential time complexity:
            O(N!)        -> Factorial time complexity 

    c. type of data structures

        1. arrays
            accessing value at a given index: O(1)
            updating value at a given index: O(1)
            inserting value at the beginning: O(n)
            inserting value in the middle: O(n) 
            inserting value at the end: 
                for dynamic arrays: O(1)
                for static arrays: O(n)
            removing value at the beginning: O(n)
            removing value in the middle: O(n) 
            removing value at the end: O(1)
            copying the array: O(n)

        2. linked lists

          SINGLY linked lists
            accessing the head: O(1)
            accessing the tail: O(n)
            inserting a node in the middle: O(n) 
            inserting / removing head: O(1)
            inserting / removing a node: O(n) to access + O(1) to operate 
            inserting / removing the tail: O(n) to access + O(1) to operate
            searcing a value: O(n)           

          DOUBLY linked lists
            accessing the head: O(1)
            accessing the tail: O(1)
            inserting a node in the middle: O(n) 
            inserting / removing head: O(1)
            inserting / removing a node: O(n) to access + O(1) to operate 
            inserting / removing the tail: O(1) 
            searcing a value: O(n)           

        3. hash Tables
            basically dictionaries
            inserting a key/value pair: O(1)
            removing a key/value pair: O(1)
            Looking up a key: O(1)
            All O(n) in worst case scenarios in which collision create long linked lists as values


        4.stacks (implemented as dynamic arrays or singly linked lists) -> LIFO
            just like a stack of books
            pushing an element onto the stack: O(1)
            popping an element off the stack: O(1)
            peeking at the element on top of the stack: O(1)
            searching for an element in the stack: O(n)
        
        5. queues (implemented as double linked lists) -> FIFO
            Just like a queue of people
            enqueuing an element into the queue: O(1) 
            dequeuing an element out of the queue O(1)
            peeking at the element at the front of the queue O(1)
            searching for an element in the queue O(n)     
        
        6. strings
            They are just like arrays of letters
            Important thing: they are immutable, therefore modyfing a string is an O(n) operation
            because everything we do something like += on a string we are actually creating a new string
            traverse a string: O(n)
            copy a string: O(n)
            get a value: O(1)

        7. graphs
            important definitions:
                connected vs unconnected
                directed vs undirected
                cyclic vs acyclic
            important operation
                representation of a graph: usually a hash table with vertices(key) and edges(value)
                storing a graph: O(V+E) space
                traversing a graph
                    depth search: you go deep first in nodes and then wide 
                        for binary trees can be done:
                            - in order
                            - pre order
                            - post order
                    breadth firts: you go to the adjacent nodes first and then deep in them

        8. trees
            basically a graph that is connected, directed, acyclic
            types of trees:
                - BYNARY trees vs K-ARY tree: binary has max 2 nodes, while k-ary has max k nodes.
                - FULL trees: nodes have either 2 or 0 child-nodes 
                - COMPLETE trees: an almost perfect binary tree. the last level is filled from left to right.
                - BALANCED trees --> left and right subtrees have a difference of at least 1 level.
                                     a balanced tree is a tree that roughly mantains a Log(n) time complexity
                - PERFECT trees: all nodes have exactly 2 child nodes and all leafs have the same depth
            main operations
                storing a tree: O(n)
                traversing an entire node: O(n)
                traversing a specific path: O(LogN)


        9. Recursion, Memoization, tabulation, Dynamic Programming

            dynamic programming:
                - any instance where we have some larger problem
                - that we can decompose into smaller instances of the same problems (subproblems)
                - and we also have an overlapping structure
                - the solution to the main problem is the result of the optimal solution of the subproblems

            dynamic programming recipe
                - notice any overlapping sub problems
                - decide what is the base case input
                - think recursively to use memoization
                - think iteratively to use tabulation

            memoization recipe
                - first: create a recursive function that works
                    - visualize the problem as a tree
                    - implement the tree using recursion
                - second: make it efficient
                    - add a memo object 
                    - add a base case to return memo values
                    - store return values into the memo

            tabulation recipe
                - visualize the problem as a table
                - size the table based on the inputs
                - initialize the table with default values
                - seed the trivial answer into the table (the base case)
                - iterate through the table 
                - fill further positions based on the current position

        10. Heaps
            a tree that is
            - complete
            - follow the heap properties, but it is not necessarly sorted
                - minheap: every node is smaller than its childrens 
                - maxheap: every node is bigger than its childrens
            - represented as an array and a tree
                - child one = 2i+1
                - child two = 2i+2
                - parent = floor ((i-1) / 2)

2) ALGO & PROBLEM SOLVING PRINCIPLES and PATTERNS

    FIRST PRINCIPLES
        1. DIVIDE AND CONQUER down to fundamental components
        2. THINK ALGORITHMICALLY / ITERATIVELY
        4. USE FOUNDATIONAL LOGIC, BASIC MATH AND SIMPLICITY
        3. START CONCEPTUALLY AND THEN CODE 
        4. START FROM CODE MVP AND OPTIMIZE

        1. State the problem / goal clearly. Identify the input & output formats.
        2. Come up with some example inputs & outputs. Try to cover all edge cases.
        3. Come up with a correct solution for the problem. State it in plain English.
        4. Implement the solution and test it using example inputs. Fix bugs, if any.
        5. Analyze the algorithm's complexity and identify inefficiencies, if any.
        6. Apply the right technique to overcome the inefficiency. Repeat steps 3 to 6.

    STRATEGIES 

        1. traverse with one or more pointers in different directions (left, right, middles..)
        2. use auxiliary hash tables, lists and daa structures
        3. use helper functions
        4. use OOP and auxiliary classes to store info and do stuff
        5. use counters and built in functions and methods
        6. sort
        7. swap and shift 
        8. recursion 
        9. bucketingg and splitting
        10.memoization + tabulation

        Notes for Linked lists, trees
        - be careful while removing or changing bindings


    A. ARRAYS

        1. TWO NUMBER SUM
            input: an array of integers and an integer k
            output: return true if the sum of two integers in the arrays are equal to k
            short solution: sort + traverse the array with 2 pointers l and r
                
            detailed solutions
                - brute force O(n2) time / O(1) space:
                    - 2 nested for loops 
                - optimized O(n) time / 0(n) space
                    - traverse array + hash table 
                    - check if sum-integer is in the hash table 
                - optimized O(nlogn) time / O(1) time:
                    - sort the array
                    - traverse the array with 2 pointers (left+right)
                    - check if left + right = k, move the pointers as consequence (move left i sum is smaller, move right if sum is bigger)       
                    
        2. VALIDATE SUBSEQUENCE
            input: 2 non empty arrays of integers
            output: return true if the second array is a subsequence of the first
                    (a set of number in the same order, not necessarely adjacent)
            short solution: set pointers in both array, move the sequence pointer if found in the array
            
            detailed solution:
                - O(n) time / O(1) space
                    - set a pointer i in the second array 
                    - traverse the first array until you find the i pointer value
                    - if you find it, move i + 1
                    - continue traversing the second array

        3. SORTED SQUARED ARRAY
            input: an ascending sorted array on integers
            output: return a new array of same lenght with the squares of the original array
            solution: traverse the array with 2 pointers l and r

            detailed solutions
                - trick conditions: negative numbers
                
                - brute force O(nlogn) time / O(n) space 
                    - traverse the array
                    - calculate the square of each integer and append them in a new array
                    - sort the new array
                
                - optimized O(n) time / O(n) space
                    - initialized an empty array b of the same lenght of array A
                    - set 2 pointers l and r in array a 
                    - compared the square of l and r value, add the biggest at the end of array B
                    - move the appended value by 1 (if r added, r-= 1 else l+=1)
                    - repeat until l = r

        4. TOURNAMENT WINNER
            input: an array of competitions[home, away], an array of results[1 if home wins, 0 if away wins]
            output: return the winner of the tournament
            solution: traverse arrays + hash table

            detailed solution:
                - O(n) time / O(team) spaces
                - traverse the competition and results arrays
                - store in a hash table the wins / pointers
                - track the highest winner team 

        5. NON CONSTRUCTIBLE change
            input: an array of positive integers (coins)
            output: return the minimum sum of money you cannot create 
            solution: sort + traverse the array

            detailed solution:
                - sort the array
                - traverse the array and calculate the sum
                - if next integers > sum + 1, return sum+1 

        6. THREE NUMBER SUM
            input: an array of integers and an integer k
            output: return all triplets of three integers in the arrays that makes the target sum k
            short solution: sort + nested for loops + traversals with l and r pointers

            detailed solutions
                - brute force O(n3) / O(n) space
                    - three for loops nested

                - optimized O(n2) time / O(n+n) space
                    - double for loops nested + hash table

                - optimized O(n2) time / 0(n) space 
                    - sort the array
                    - traverse it with for loop + nested traversing with a l and r pointers
                
        7. SMALLEST DIFFERENCE
            input: 2 arrays of integers
            output: find and return the pair of numbers (one from each array) whose abs difference is closest to zero
            short solution: sort and traverse with pointers one at the time

            detailed solutions
                - brute force O(n2) / O(1) time
                    - 2 nested for loops with all combinations

                - optimized O(nlogn)+O(mlogn) time / O(1) time
                    - sort the arrays
                    - traverse the 2 arrays with a pointer, move a pointer at the time

        8. MOVE ELEMENT TO END
            input: an array of integers and an integer k
            output: move all instances of k at the end of the array, return the array
            short solution: traverse the array with 2 pointers l and r, swap them

            detailed solution:
                - O(n) time / O(1) space
                - traverse array with 2 pointers l and right
                - if r == k, move r to - 1 
                - if l is == k, swap it with r
                - move l to + 1

        9. LONGEST PEAK
            input: an array of integers
            output: return the leght of the longest peak
            short solution: traverse the array + move l and r pointers outwards

            detailed solution O(n) time / O(1) space
                - first find all of the peaks    
                    - traverse the array with a main pointer
                    - check l and r pointer to check the peak
                - second find the longest one
                    - position yourself at the first peak
                    - move l and r outwards to see how long the condition is respected
                    - do it for the next peak, until you finish them
                    - compare peak lenghts and retur

    B. STRINGS

        1. PALINDROME CHECK
            input: a string
            output: return True isf the string is a palindrome
            short solution: traverse with 2 pointers
                
            detailed solutions
                - brute force O(n2) time / O(n) space
                    - traverse from right to left the string + append it in a new string + final compare
                - brute force O(n) time / O(n) space
                    - traverse from right to left the string + append it in a list + concatenate and final compare
                - Optimized O(n) time / O(n) space  
                    - compare l and r letters + recursively do it with the middle string.
                - optimized O(n) time / O(1) space
                    - traverse with 2 pointer l and r + compare each char

        2. CAESAR CIPHER ENCRYPTOR
            input: a string and a integer k
            output: return a new string made by shifting the letters in the string by key position in the alphabet
            short solution: traverse and use unicode values + modulo operators
                
            detailed solutions
                - O(n) time / O(n) space
                - use unicode values ord(letter) + K
                - use the modulo operator to wrap around 97 and 122 (96 + new letter code % 122)

        
        3. RUN-LENGTH ENCODING
            input:a string
            output: return its run lenght encoding (eg AAAA -> 4A, with an encoding up to 9)
            short solution: traverse the string + auxiliary list
                
            detailed solutions
                - traverse the string and calculate lenght of run
                - store lenght and char in a list

        4. GENERAT DOCUMENT
            input:a string of characters and a string represneting a document
            output: determine true / false if you can generate the docu using the available characters 
            short solution: traverse the string + auxiliary hash table
                
            detailed solutions O(n+m) time / O(1) space
                - traverse the string and store key values in a dict
                - traverse the document and check in the dicts

        5. FIRST NON REPEATING CHARACTER
            input:a string of characters 
            output: return the index of the first non repeating character 
            short solution: traverse the string + hash table 
                
            detailed solutions O(n) time / O(1) space
                - traverse the string and store key values in a dict, return the first one to get a value of 2

        6. LONGEST PALINDROMIC SUBSTRING
            input: a string
            output: return its longest palindromic substring  
            short solution: traverse with 2 pointers
                
            detailed solutions O(n2) time / O(1 or n) space
                - find all palindrome by traversing the string with 2 pointers
                - finde the longest one by storing it in a variable 

                
        7. GROUP ANAGRAMS
            input: a list of strings
            output: return a list of anagrams groups
            short solution: traverse + hash table + sort
                
            detailed solutions
                optimized: 0(nlogn) / O(n)
                - traverse the list, sort the strings
                - use hash table to check if they are anagrams  

        8. MINIMUM CHARS FOR WORDS
            input: an array of words
            output: return the smallest array of characters needed to form the words
            short solution: 
                
            detailed solutions
                optimized: 0(nlogn) / O(n)
                - traverse the list, sort the strings
                - use hash table to check if they are anagrams 

    C. SEARCH & SORTED

        1. BINARY SEARCH
            input: a sorted list and a integer k
            output: use binary search to determine if k is in the list 
            short solution: sort + pointers l, m, r

            detailed solutions
                - O(logn) / O(1)
                - left, middle and right pointers
                - if k > middle -> go right
                - if k < middle -> go left

        2. BUBBLE SORT 
            input: a list of integers
            output: return the sorted list using bubble sort
            short solution: traverse multiple times the array and swap i and i+1 if necessary
                            O(n2) / O(i)

        3. INSERTION SORT
            input: a list of integers
            output: return the sorted list using bubble sort
            short solution: traverse the array, at each index swap the number back to current position 
                            until beginning of array or correct position
                            O(n2) / O(i)

        4. SELECTION SORT
            input: a list of integers
            output: return the sorted list using bubble sort
            short solution: traverse the array, at each index traverse again and find the smallest value
                            swap it at the index position
                            O(n2) / O(i)

        5. 3 NUMBERS SORT
            input: an array of 3 distinct integers + an array of integers
            output: sort the array following the order of the 3 integers, in place
            short solution: traverse with 3 pointers and swap stuff
                            O(N) / O(1)

        6. QUICKSORT
            input: an array
            output: return the sorted array using quicksort
            short solution: pick a pivot, a left and right pointer, traverse and swap them
                            recursively apply the same tecnique to le left and right subarray of the pivot
                            O(nlogn) / O(logn)

        7. MERGESORT
            input: array
            output: return the sorted array using MERGESORT
            short solution 1, recursion: 
                - divide and conquer the array in half
                - do it recursively until you get to single numbers
                - merge them back in sorted order
                - use auxiliary arrays at every level as support
                - O(nlogn) / O(nlogn)
            short solution 2, recursion with swap
                - create only 1 auxiliary list
                - divie and conquer the array in half
                - recursively apply the merge sort helper swapping main and auxiliary list
                - merge them back directly in the main array 

    D. RECURSION
        
        1. NTH FIBONACCI
            input: an integer
            output: return the nth fibonacci number
            short solution: 
                
            detailed solutions
                - Fibonacci sequence = 0, 1, 1, 2, 3, 5 etc... where the nth number is = n-1 + n-2 number
                - first number is 0, second number is 1

                recursion brute force O(2^n) / 0(n)
                    - set the fundamental conditions: if n == 1 or 2 return 1, 2
                    - recursively return fibonacci function on n-1 and on n-2

                recursion optimized O(n) / O(n)
                    - create auxiliary hash table with key = position and value = fibonacci value
                    - if n in the hash table, return its value
                    - else recursivel call fibfunction on n-1 and n-2

                iteratively with auxiliary list O(n) / O(n) 
                    - store items in a list
                    - while loop untile len of the list is = N
                    - append new values calculated as n-1 + n-2
                
                iteratively wit fixed list O(n) / O(1)
                    - create a list of 2 items storing just the last 2 numbers of the sequence
                    - set a counter starting from 3
                    - calculate the next number with while loop until counter = N
                    - at each iteration shift by -1 the second item in the list and add the last number in the sequence

        2. PRODUCT SUM
            input: a special array
            output: return the sum of items multiplied by their level of depth
            short solution: traverse the array + recursion

            detailed solution:
                - traverse the array
                - if the element is an integer, add it to the sum
                - if it is an array, recursively cll the function with the multiplier +=1
                - O(n) / O(d)


        3. PERMUTATIONS
            input: an array of integers 
            output: return an array of arrays with all their permutations
            short solution:
                - iterative: double for loop concatenating array[value] with the subset list
                - recursive: call recursively the function increasing the index of the array. 
                             first concatenate with the subset and then call recursively (so that the subset list is updated)
                             or start from the end of the array, first call recursively and then append (so that the stack is updated)

                - O(n*2^n) / O(n*2^n)
        
        4. LOWEST COMMON MANAGER
            input:3 instances of an OrgChart class that have a directReports property. first input is the top manager, the other 2 are reports
            output: return the lowest common manager to the 2 reports
            short solution: 
                - dfs traverse the tree like structure starting from the root node
                - use depth first search, going down 1 subtree at the time
                - count how many matching report a node has
                - if the result is 2 return that node
                - O(n) / O(d)

        5. INTERWEAVING STRINGS
            input: 3 strings 
            output: return a boolean true if the 3rd string can be formed interweaving string 1 and 2
            solution: recursion with caching

    E. LINKED LISTS, QUEUES AND STACKS

        1. REMOVE DUPLICATE VALUES FROM LINKED LIST
            input: a sorted linked list
            output: a linked lists without duplicate values
            short solution: 
                - traverse the linked list.
                - when node.value = node.prev.value change pointers and remove the node
                - continue
                - O(n), time


        2. REMOVE Kth NODE FROM END
            input: a linked list, a nr n
            output: the ll withouth the nth node from the end. don't retun anything
            short solution: 
                - traverse with 2 pointers
                - move the first one by N
                - then start moving both of them until the first one get to the last node
                - now the second one is pointing to the nth node
                - remove it 
                - return the ll
                - O(n), time

        3. FIND LOOP IN A LINKED LIST
            input: alinked list
            output: return the node from which the loop originates
            short solution:
                - traverse with 2 pointers
                - first pointer moves one node at the time
                - second node moves two pointers at the time
                - when the two pointers meet at the same node
                - that's the node from which the loop originates
                - O(n) / O(1)

        4. REVERSE A LINKED LIST
            input: a linked list
            output: return tre reversed linked list in place
            short solution:
                - check extreme condition: head is the only node
                - traverse with 3 pointers
                - change the second.next pointer to the first node
                - move pointers up by one and repeat
                - untile second pointer is not none
                - then the second is at the end
                - set it as the head

        5. MERGE 2 LINKED LISTS
            input: 2 sorted linked list
            output: return a merged linked list in place
            short solution: 
                - traverse linked lists with 3 pointers. P1 Prev1 P2 
                - Compare P1 and P2 and swap Prev1 -> P2, P2 -> P2.next, Prev1.next -> P2
                - edge case: set the head correctly
                - O(n+m) / O(n)

        6. SHIFT LINKED LIST
            input: a ll and an integer k
            output: shift the list in place by k positions and return the new head
            short solution:
                - k = positive
                - set 4 variable: head, tail, newhead, newtail
                - head is head, tail is set to k, newh, newt are head -> move by 1 tail, newh, newt until you reach the end
                - swap them
                - edge cases
                    - k = 0, return
                    - k is bigger than ll, use modulo
                    - k is negative

        7. BALANCED BRAKETS
            input: a string of brackets ()[]{}
            output: return True id the string is balanced 
            short solution: build a stack to solve it
                - balanced = all opening brackets match the closing ones of the same type, without overlappping
                - build a stack to verify all brackets
                - classify brackets by open vs closed and type
                - if open, add it to the stack
                - verify extreme cases that breaks the balancing
                    - len is odd, false
                    - if closed by of different type from the peek, false 
                    - if at the end of the string there are still items in the stack

        8. SUNSET VIEW 
            input: an array of integers representing building heights + a direction east or west
            output: return an array of indices that can see the sunset
            short solution:
                - building can see the sunset if there are no other buildings taller of equal to them in the direction
                - right is east
                - left is west
                - traverse in reverse order to the direction
                - if the next building is taller than peek, append buildings to the stack
                - update the stack
                - edge cae: the first building. if stack is empty

        9. SORT STACK IN PLACE RECURSIVELY
            input: an array 
            output: sort the stack in place and return it 
            short solution:
                - recursively pop and insert back all elements from the stack
                - insert back if the nr is bigger than the peek
                - else pop the peek and insert the number

        10. FIND NEXT GREATER ELEMENT
            input: an array 
            output: another array with the next greater element index at each position compared to the original array
            short solution:
                - traverse the array
                - for each i create a circular i with modulo
                - for each i compare the value at i with the peek with a while loop
                - if greater, pop the peek
                - add the index in the result array
                - then in any case add the value to the stack

    F. TREES

        1. BST CONSTRUCTION
            input: nothing
            output: write a bst class with insertion, searching, deltion
            short solution:
                - Inseartion:
                    - move to the right or left node until you find the right spot (null value)
                    - add the new node
                - searching
                    . move to the l or r nodes until you find the node you are looking for
                    - if it is not present, return false
                - Deletion
                    - move to the l or r node until you find the node to delete
                    - when there, deal with edge cases:
                        - no children, just remove it, set the parent node pointer to null
                        - 1 children, remove it and switch parent with node to remove
                        - 2 children, remove it, find the smallest value in the right subtree and swap it with the parent

        2. BST - FIND CLOSEST VALUE
            - input: a bst and an integer value k
            - output: return the closest value to k in the bst 
            - edge cases:
                - negative numbers
                - multiple times the same closest value
            - short solution:
                - traverse the bst moving to the l and r node
                - keep a variable closest to store the current closest value
                - compare the abs difference of the left and right node to the current closesnt
                - move in the direction of the closest one
                - O(logn) / O(1)

        3. BST - VALIDATE BST
            - input: a potentially binary search tree 
            - output: return t or f. validate if it is a bst  
            - edge cases:
                - only 1 root node
            - short solution: recursive
                - we can do i recursively
                - set a min and max values, fro every node
                - traverse each node going l or rais
                - if the value is out of the min or max return F, else T 

        4. BST - MIN HEIGHT BST
            - input: a sorted array of integers
            - output: build a min height bst from the array
            - edge cases: same number, just 1 number, empty array, 
            - short solution: recursive
                - use a function similar to binary search
                - find the middle of the array
                - use it as the root node / insert it
                - define the left and right side using start and end variables
                - recursively do it until start > end

        5. BST - FIND KTH LARGEST VALUE
            - input: a bst and an integer K
            - output: find the kth largest value in the bst
            - edge cases: k is the root
            - short solution:
                - traverse with a reversed in oder method the bst (right, current, left)
                - set a counter starting from 1
                - when counter is equal to k, return the node.value / append.it to a list
                - use a class for the counter

        6. BST - RECONSTRUCT A BST
            - input: an array of integers representing the values of nodes read with a pre-order traversal
            - output: reconstruct and return the bst from the array
            - edge cases: array is empty, array is just 1, array is a linked list
            - short solution: 
                - traverse the array with a class index info.
                - set updating upper and lower bound
                - at each element try adding it both to left and right by matching it with upper and lower bound recursively
                - base case is if you are at the end of the list, return none
                - base case is if you are out of the boundaries, return none
        
        7. BINARY TREE - BRANCH SUM 
            - input: a binary tree
            - output: return a list of the sums of its branch values
            - edge cases: 
            - solution: 
                - depth first search traverse
                - at each node update a sum variable
                - recursively move to the l and r node
                - base case: if there are no children add the sum to the list 
                - O(n) / O(n)

        8. BINART TREE - NODE DEPTH sums
            - input: a binary tree 
            - output: return the sum of the depth of all its node from the root 
            - edge cases: only 1 node,
            - solution: 
                - depth first search traverse
                - use a sum variable to keep track of the sum
                - recursively apply the traverse to left and right nodes
                - at each call increase depth + 1
                - base case, if node is none, meaning root or end of the tree, return 0

        9. BINARY TREE- INVERT BINARY TREE
            - input: a binar tree 
            - output: return the reversed binary tree left with right 
            - edge cases:
            - solution:
                - breadth first search
                - at each node swap left and right child
                - do it recursively 
                - base case is if node is none, return nothing
                - return the tree at the end

        10. BINARY TREE - FIND SUCCESSOR
            - Input: a biary tree and a node 
            - output: return the successor of the node when visited with inorder traversal
            - edge cases: there is no successor (the node is the last one visited)
            - solution:
                - in order traverse and store elements in a list
                - traverse the list
                    - when you find the node, return its next number
                    - if you don't find it, return none

                - use the parent
                    - if the node has a right child
                        - return the left most child in the right subtree
                    - else
                        - return the parent that have the node in its left subtree

        11. BINARY TREE DIAMETER
            - input: a binary tree 
            - output: return its diameter (the lenght of its longest path)
            - edge cases: 
            - solution:
                - recursively traverse with dfs the tree and store in a variable the longest path 
                - the longest path is the max between left subtree, right subtree and the sum of them at their root node  
                - use a class tree info to store this infos
                - O(n) / O(h)

        12. HEIGHT BALANCED BINARY TREE 
            - input: the root node of a binary tree 
            - output: return true if is height balanced = for each node the difference in height of the left and right subtree is at most 1
            - edge cases: 
            - solution:
                - o(N) / O(h)
                - recursivelu traverse dfs the tree
                - create an auxiliary class to store a boolean is balanced and height of each node
                - base case is if node is none, return true and -1 as height
                - at each node check if isbalanced (both left and right subtree info are balanced) and the difference in height of left and right is no more than 1
                - at each node update the height

    G. GRAPHS 

        1. DEPTH FIRST SEARCH 
            input: a node of a graph 
            output: implement dfs from left to right, store all nodes name in an array and return it 
            edge cases: only 1 node, 
            short solution:
                - loop visit the node
                - go deep into its children from left to right before moving to the next node
                - do it recursively
                - O(v+e) / O(v)

        2. BREADTH FIRST SEARCH
            input: a node of a graph
            output: implement bfs from left to right, store all nodes in an array
            edge cases:
            short solution:
                - start from first node
                - loop visit all the children
                - recursively do it to their children
                - O(v+e) / O(v)

        3. CHECK CYCLE
            input: an array of integers
            output: return true if they form a CYCLE wrapping around the array 
            edge cases: negative and positive numbers, 1 or 0 item list
            solution:
                - traverse the array and check if there are false conditions
                - condition 1: we have checked more than 1 item and we are back to starting index
                - condition 2: we have completed the traversal but we are not back to position 1
        
        4. YOUNGEST COMMON ANCESTOR
            input: a tree like charts and 2 descendant nodes
            output: find and return the youngest common ancestor
            edge case: only 1 or 2 items, one of the 2 descendant is the ancestor, different lenght 
            solution:
            - find the depth of both of the descendants by going back up until the root node
            - level them to the same depth
            - move them up by 1 until their are the same node, which is the common ancestor
            - O(d) / O(1)

        5. CYCLE IN GRAPH
            input: a list of edges reresenting a graph 
            output: return true if the graph contains a cycle
            edge case: 
            solution:
                - trverse with dfs + auxiliary list to store if a node has been visited and its in the stack
                - create 2 lists, visited and stack
                - traverse with dfs
                - if node is leaf, mark as visited but out of the stack
                - if node has child, mark as visited and in the stack
                - if a node points to a node in the stack, return true
                - else return false
                - O(v+e) / O(v)
                - can also be done with colors / numbers 0, 1 , 3 

        6. REMOVE ISLANDS
            input: a matrix (a 2d array) made of 1 and 0s
            output: return a modified version of the matrix where islands are removed
                    islands are group of horizontally or vertically adjacent 1 (not diagonal) that don't touch the bords
            solution:
                - create an auxiliary matrix 
                - traverse the borders of the matrix and mark as true the 1s that are connected to the borders
                - use dfs to goo deep into an islands
                - then traverse the inside of the matrix and switch to 0 all the ones that are false
                or
                - traverse the border and transform all 1s connected to the border into 2
                - use dfs to go deep into an islands
                - traverse again the matrix and swtich all 2s into 1s and all 1s into 0s.

        7.  MINIMUM PASSES OF matrix
            input: a 2d matrix of positive and negative integers
            output: return the minimum number of passes to convert negative into positive numbers
                    a negative can be converted only if there is an adjacent positive (up, dow, left, right)
            edge cases: only negative, only positive, only 1 positive
            solution:
                - traverse the matrix + bfs + queue
                - traverse the matrix and find all positive values
                - store them in a queue
                - one by one, transform all their adjacent negative into positive
                - store the new positive in a second queue
                - repeat by swapping the queues
                - return the number of passes with a counter variable (each time we empty a queue)

    H. DYNAMIC PROGRAMMING
        
        1. MAX SUBSET SUM NO ADJACENT 
            input: an array of positive integers 
            output: return the maximum sum of non adjacent elements 
            edge cases: empty array return 0, keep track of indexes, stings and negative numbers 
            short solution:
                - traverse the array with an auxiliary array storing the current max 
                - initialize a new array with position 0 and position 1, which is the max of pos 1 and 0
                - traverse the original array
                - at each number add in the aux array the max of the previous max (max[i-1]) which is non-adjacent and the max[i-2] + array[i]
                - return the last position in the auxiliary array
                - O(n) / O(n)
                - can be optimized by using only a list of 2 position to bring space complexity to O(1)

        2. NUMBER OF WAYS TO MACKE change
            input: an array of positive integers representing coins + an integer n positive representing a target amount of money
            output: return an integers representing the number of ways to make change for that target n using the given coins (unlimited number per typer available)
            edge cases: no ways to make change 
            short solution: 
                - traverse the array and use an auxiliary list to store the nr of ways
                - initialize an arrya that has n position and represent the nr of ways to make change
                - the value at position 0 is 1 (there is only 1 way to change 0, which is no coin)
                - traverse the array and at each coin traverse the aux array.
                - at each position of the aux array, if the coin is > than the position index, move on
                - if the coin is >= than the position index, update the nr of ways with ways[i] += ways[i-coinvalue]
                - return the last position in the aux array
                - O(nd) / O(n)

        3. MIN NUMBER OF COINS FOR CHANGE
            input: an array of positive integers representing coins + an integer k representing a target amount
            output: return the smallest number of coins needed to make a chnge
            edge cases:  
            short solution: 
                - traverse the array and use an auxiliary list to store the minimum nr of coins to change
                - initialize an arrya that has n position and represent the min nr of coins to make change
                - the value at position 0 is 1 (there is only 1 way to change 0, which is no coin)
                - traverse the array and at each coin traverse the aux array.
                - at each position of the aux array, if the coin is > than the position index, move on
                - if the coin is <= than the position index, update the min nr of coins  with mincoins[i] = min (mincoins[i], mincoins[i-coin])
                - return the last position in the aux array
                - O(nd) / O(n)

        4. LEVENSHTEIN DISTANCE 
            input: 2 strings
            output: return the nr of necessary operations to make string 1 equal to string2 (insertion, deletion, substitution)
            edge cases: equal strings, no strings, special characters..
            short solution:
                - build an NM matrix starting from empty strings
                - base case is 0, emptu string is equal to empry string
                - traverse the matrix from left to right
                - each new slot is equal to:
                    - if char[r] == char[c] --> char[r][c] = char[r-1][c-1]
                    - if char[r] == char[c] --> char[r][c] = 1 + the minimum of the adjacent slots (above, left and left-above)
                - return the last slot
                - can also be done using only the last 2 columns
                - O(NM) / O(nm)

        5. GRID TRAVELER PROBLEM
            input: 2 positive integers representing w and h of a grid shap graph 
            output: return the nr of ways to reach the bottom right corner from the top left corner, moving only right or down 
            edge cases: minimum grid 2x2, grid 1,1, 0 and 0, very large numbers 
            short solution:
                - recursive
                    - recursively calculate the nr of ways to get to the slot above and left of the the final slot
                    - the base case is the first slot when w and h are 1 
                    - can aalso use memoization to make it cleaner 
                    - O(2 ^ m*n) / O(nm)
                - dynamic programming
                    - create an auxiliary grid
                    - set the base cases to 1 in the first row and first column, because they are all 1x1 grids
                    - traverse the grid and add in the cell the sum of ways in the grid above and left 
                    - O(nm) / O(nm)
                - math 
                    - he nr of ways to reach the final slot is the number of permutations possibile of nr of moves right + nr of moves down
                    - nr of moves possible K = width-1 + height-1
                    - the number of permutation of K = factorial of k! / factorial of movesR! + factorial of movesD! 
                    - O(n+m) / O(1)

3) SYSTEM DESIGN (to clean up)

    a. CLIENT - SERVER MODEL


    b. NETWORK PROTOCOLS
        IP                      -> internet protocol 
        IP packets              -> made by IP HEADER + DATA
        TCP                     -> transmission control protocol -> adds a TCP HEADER to the IP packets 
        HTTP                    -> Hyper text transfer protocol


    c. STORAGE
        disk vs memory + persistence


    d. LATENCY AND THROUGHPUT 
        latency: how long it takes for data to traverse a system, more specifically ho long to go from one point to another in the system. aka to complete and operation
                 usuakky measured by gb x seconds, mb x seconds etc
                 latency / speed depends primarly on 2 things: the type of source in the system we are transfering to/from. eg. Memory -> SDD -> HDD
                 the distance of transfer to / from the network. eg. client - servers in the same state vs intercontinental client - server
        throughput: the number of operations that a system can handle properly per time unit. for example the throughput of a server can be measured by request per seconds.
                    how to increase throughput? simple, you pay more and upgrade your server provider with more / better servers


    e. AVAILABILITY
        Measured in nines - 99% = 2 nines
        HA = High Availability, 5 or more nines
        Its a critical components of system architectures
        SLA and SLO of services like AWS - Service level agreement, based on service level objectives
        REDUNDANCY = reducing the risk of single risk of failure (= having a part of the system that depends on a single machine/components of the system)
                     by multiplying machine / parts of the system in order to make the system more reliable
                     eg. multiple servers / 2 engine planes etc...
                     it is basically the concept of diversifying the risk of system failure to make the system better and more available


    f. CACHING        
        used to speed up a system by improving/reducing the latency of system  
        also used in algorithms to improve time complexity by avoiding to do complex operations multiple times
        caching = storing data in a new location that is different from the location where it was originally stored, such that it is faster to retrieve this data
        examples of use of caching
            avoiding to do a network request multiple times by caching its results
            avoiding to perform a big time complexity operation on a server by caching its results
            avoiding a datbase overload with multiple requests by caching its data somewhere else
            ...
        there can be READ-ONLY cache or also WRITE cache  
        Staleness: caches can become stale if they haven't been updated proprerly
        Eviction Policy = the protocol you use to remove data from a cache. most famous are
            LFU: Last frequently used 
            LRU: Last recently used 
            FIFO: firs in first outside


    g. PROXIES
        Forward Proxies vs Reverse Proxies
            Forward proxies = servers that are placed between clients and servers and more specifically servers that act on behalf of the clients
                              basically how vpn works in a symplified way
            Reverse proxies = servers placed between clients and servers and more specifically servers that act on behalf of the servers
                              tricky but very useful. for ex. used to filter request to the real server or used as a load balancer or cache
    
    h. LOAD BALANCERS
        A server between client and servers that has the job to balance and distribute requests/traffic between servers efficiently
        Used in basically every it system
        benefits of a load balancer
            reduced risk of server overload
            better throughtput
            better latencies
        load balancers are basically a kind of reverse proxy
        there are both SOFTWARE LOAD BALANCERS and HARDWARE LOAD BALANCERS
        software lod balancers are almost always more common and efficient
        a system can have multiple load balancer and different server selection strategies applied


        Server seleciton strategy = the protocol that the load balancer applies to distribute traffic to the servers. most used are
            round robin = in order from first to last available
            weighted round robin = from first to last but weighted based on stuff like power 
            performance based = from best performing to last performing (in terms of speed, capacity, cost efficiency)
            random = 
            IP based = for example by associating ip with specific servers (ip from italy with servers from italy)
            path based = distribution based on the path of the request. so basically distributing by area of the product / system, efficient for operational purposes and releases for examples.
                         eg. all payment request to server 1, all code request to server 2, all ....


    i. HASHING
        Hashing function is a function that takes in data, typically a string or identifier, and transform it in an integer minimizing hashing collision
        Hashing in system design are useful in load balancers to distribute efficiently and in a scalable / adaptable way
        For example by assiciating clients to servers through hashing and maximizing the use of cashing.
        2 very commonly used hashing methods
            CONSISTENT HASHING = A type of hashing usually represented in a circle. it maximize the nr of keys that mantains consitency when a table gets remapped, especially in case a new server gets added
            RANDEZVOUS HASHING = puts ranking order for clients and servers and associate them by ranking order. it maximize consistency if a server goes down because only its clients will be remapped to the next higher ranking server.


    l. RELATIONAL databases
        Relational databases = called also sequel or SQL databases = a type of structured databases in which data is stored in tabular form, with rows and columns
        Non relational databases = a type of databases in which data is not structured in a tabular form.
        SQL = structured query language, is the language used to query data from a sql database
        ACID Transaction: an operation in a database that has 4 proprerties
            Atomicity: the sub-operations that constitute a transaction will all succeed or all fail. if one fails, all fail.
            Consistency: all transaction in a database follow and respect the rules of the database. and all transaction takes into consideration the past transactions. there is no stale state in a db.
            Isolation: the execution of multiple transactions concurrently will have the same effect as if they have been executed sequentially.
            Durability: when you perform a transaction on a database. the effects of that transactions are permanent.
        Database Indexes are fundamental for reading speed     

    m. KEY-VALUE STORES 
        a popular type of nosql database that let you store data in key-value pairs
        typically used in databases or parts of databases where you don't need a structured table form or where data is very simple
        for examples. in caching where request are associated in pairs with specific data responses.
        
    n. SPECIALIZED STORAGE PARADIGMS

        Blobstore
            blob = arbitrary piece of unstructured data
            unstructured data = eg. images, big binary numbers ...
            blobstores are databases that specializes in storing and retrieving massive amounts of unstructured data 
        
        Time series database
            databases that specialize in storing and retrieving time series data
            expecially if the time series is very tight. for example trading operations

        Graph databases
            a database that is built on top of the graph datamodel.
            the core of the graph database structure is the relationship between datapoints
            instead in an sql db the relationship is implied and important but not the core feature
            for examples. a graph database is perfect for storing and quering social media connections

        Spatial databases + quadtree
            databases that relies on spatial indexes 
            for example locations on a map
            a lot of special indexes relies on trees 
            quadtree is a tree in which every node has 0 or 4 children nodes, basically a quadtree is a grid and all dots represent a location in a map


    REPLICATION AND SHARDING
        Replication = the act of replicating the data on the main database in another database called replicate
                      can be useful to decrease single risk failure
                      can be also used to improve latency, by for example placing the replica near the client

        shards = called also data partitioning. sharding is the act of splitting the database into more pieces called shards
                 a database can be split following multiple logics. for examples
                    sharding by region / location (a user database is split by country)
                    sharding by type of data or function in a system (eg. shards for payments, shards for logistics, ....)
                    sharding by hash column, this is fundamental to avoid hotspot and overloeads 
                it is useful to increase throughput and avoid dubplicating huge and not efficient amount of data

    LEADER ELECTION
        The process through with a group server elect a leader among them via a consensus algorithm
        the leader is the server that performs the primary operations
        if the leader fall down servers immediately elect a new leader
        two popular consensus algorithms are PAXOS and RAFT
        2 key-value stores programs that apply these consensus algorithms are ETCD and ZOOKEEPER

    POLLING AND STREAMING
        two methods of communication between clients and servers, usually applied when request are done on a recurring basis on a pool of data that changes continuously
            Polling: client issues a request at set intervant and the server continuously respond
            Streaming: the act of continuously feed data to the client. so no request but  open connection (with a socket) with continuous feed/push of data -> close the connection

    CONFIGURATION
        configuration is basically a set of parameters and constants that are critical to the system 
        but instead of hardcoding them into your application code, they are added to configuration files usually written in JSON or YAML
        the vonfiguration file si usually called config.
        we can have both STATIC configuration or DYNAMIC configurations
        STATIC are hard coded and shipped with the application code. if you change them, you have to re-run the code to see the changes live.
        DYNAMIC ones live outside of the application code and if you change them their changes will be live immediately

    RATE LIMITING
        Limiing the amount of operations that can be performed in a certain amount of time on a server
        if you g beyond the limit the server will issue an error
        you can rate limit on the base of various logics: for example requests x users, x region, ...
        very important to protect your system from DOS / DDOS attacks 
        REDIS is a very popular key-value database used to perform rate limiting and cache
    
    LOGGING AND MONITORING
        Logging: the act of collecting and storing logs = useful information about events in your systems.
        How it works
            a. you use your code to create logs (inpython with print for example)
            b. one of the most used language to record logs is JSON
            c. you use a service (eg. google stackdriver) to record logs in a database so that you can check them in case of bugs

        Monitoring: the process of having visibility into a system's performance / key metrics
                    typically implemented by collecting important events in a system (logging) and then transforming them into human readable charts 

        Alerting: the process through which a system administrator gets notified if there is an issue in the system (based on logging, monitoring and threshold)
                  alerting can be connected to communication channels such as email or slack

    
    PUB/SUB SYSTEM
        a system model that is composed by  
            - publishers                    -> these are the servers. their job is to publish data to the channels
            - subscribers                   -> these are the clients. they subscribe to the channels instead of connecting to the servers.
            - topics, also called channels  -> 
            - messages                      -> some form of data or information passed from the server to the channel and received by subscribers.

        idempotent operation : an operation that has the same outcome no matter how many times it is performed

        typically operations performed through a pub-sub system should be idempotent because it is very probable that a message will go through a channel multiple times 


    MAP REDUCE SYSTEMS
        A framework for processing very large datasets spread over hundreds or thousands of machines (so in a large scale distributed system)
        in an efficient way and fault tolerant way
        there are 3 main steps
            The MAP step is based on the map function that transform data in key-value pairs, these are intermediate data.
            The SHUFFLE step, in which the key-value pairs are "shuffled" and reorganized such as pairs of the same key are routed to the same machine in the final output
            The REDUCE step, which runs a reduce function on the newly shuffled key-value pairs and transform them into more meaningful data.
        the map and reduce function must be idempotent
        
    API DESIGN 
        


D - DATA SCIENCE AND ANALYSIS 
-------------------------------------

1) DATA ANALYSYS PROCESS AND FUNDAMENTALS
    
    - Its circular

    - set the goal, input / output structure
    - Get the data and put it into the right data structure (usually a panda df)

    - Data preparation and cleaning
        1. Load the dataset into a data frame using Pandas
        2. Explore the number of rows & columns, ranges of values etc.
        3. Handle missing, incorrect and invalid data
        4. perform any additional steps (parsing dates, creating additional columns, merging multiple dataset etc.)
    
    - Data exploration and visualization
        1. Compute the mean, sum, range and other interesting statistics for numeric columns
        2. Explore distributions of numeric columns using histograms etc.
        3. Explore relationship between columns using scatter plots, bar charts etc.
        4. Make a note of interesting insights from the exploratory analysis

    - Data Analysis, inference and conclusions
        1. Identify important questions and goals  about your dataset
        2. Create new columns and further model the data by merging multiple dataset and performing grouping/aggregation wherever necessary
        3. Answer the questions and indentify the relevant insights either by computing the results using Numpy/Pandas or by plotting graphs using Matplotlib/Seaborn

    - Action 
        - Machine learning models
        - reports , stories, actionable insights,
        - other stuff
    
2) JUPYTER LABS AND JUPYTER NOTEBOOKS
    - Its the IDE suited for data analysis
    - it works in an interactive mode
    -  each piece of code is performed and the data can be immediately visualized
    
    - a         -> create a cell above
    - b         -> create a cell below
    - y/m       -> change cell type: y is python code / m is markdown
    - ctr+enter -> run the cell  


    shortcuts 

    Always working
        Shift + Enter       -> run the current cell, select below
        Ctrl + Enter        -> run selected cells
        Alt + Enter         -> run the current cell, insert below
        Ctrl + S            -> save and checkpoint

    While in command mode (press Esc to activate):
        Enter               -> take you into edit mode
        H                   -> show all shortcuts
        Up                  -> select cell above
        Down                -> select cell below
        Shift + Up          -> extend selected cells above
        Shift + Down        -> extend selected cells below
        A                   -> insert cell above
        B                   -> insert cell below
        X                   -> cut selected cells
        C                   -> copy selected cells
        V                   -> paste cells below
        Shift + V           -> paste cells above
        D, D                -> (press the key twice) delete selected cells
        Z                   -> undo 
        Y                   -> change the cell type to Code
        M                   -> change the cell type to Markdown
        P                   -> open the command / search palette.
        Space               -> scroll notebook down
    
    While in edit mode (pressEnter to activate)
        Esc                 -> take you into command mode
        Tab                 -> code completion or indent

    to retrieve information on a function or object
        func??

3) NUMPY

    import numpy

    numpy fundamentals 
        - its python package dedicated to working with numeric processes in a memory efficient and fast way
        - it mainly deals with pure int and float, arrays
        - the main difference is
            - with classic python all these types are objects and use a lot of memory (eg 20 bytes for the integer 5)
            - with numpy they are just numbers and arrays for example
        - in numpy you can decide the size of the integer, float ecc in bits to manage efficiency

    numpy arrays 
        - they work just like python lists but any operation on a numpy arrays returns a new array
        - they are however mutable and you can change values inside of it
        - with numpy array you can specify the contant type and memory size
        - with numpy arrays you cna use multi-indexing. eg

    creating an array
        a = np.arraay([1, 3, 6, 3, 6, 7, 9])       --> numpy array

    Get main info 
        .shape                                     -> return the shape of the matrix (rows, columns) (2,3) or (dimension1, dimension2 .....elements)
        .ndim                                      -> return number of dimensions (2)
        .size                                      -> return size of the matrix (6)
        .itemsize                                  -> return the lenght og one array element in bytes      
        .dtype                                     -> return the type of the object
        .nbytes                                    -> how many bytes are we using inside the memory

    numpy matrixes
        - numpy works very well and has custom functions to deal with matrixes
        - matrixes are multiple arrays inside a np.array. 
        - arrays must be of the same lenght

        axis = 0 operations are applied vertically on the columns
        axis = 1 operations are applied horizontally on the rows

        a = numpy.array([[1, 2, 3]
                        [4, 5, 6]])
        
        multi-indexing with matrixes
            a[d1, d2, d3 ....] 
            a[1][0] -> first row, index 0 -> 4
            a[1, 0] -> with multiindexing
        
        assign values to positions
            a[1] = [10, 10, 10] -> assign new values to row 1
            a[1] = 10           -> can also be done as  -> numpy matrix keep the dimension fixed
        
    Broadcasting and array operations
        - these are operations on the numpy arrays, created to be extremely fast
        - the operations are applied to each single element of the array
        - since numpy arrays are immutable, these ops return a new array with the new elements
        - vectorized operations have their python cousin in the list comprehension feature

        broadcasting : arithmatic operations between arrays of different nr of dimensions but shape compatible
        scalar operations: arithmetic operations with single numbers
        
        operators: + - * / // % **

            a = np.array([0, 1, 2, 3])
            b = np.array([10, 11, 12, 13])
        
        single number-scalar
            a + 10                  -> array([10, 11, 12, 13])  / arithmetic operations with a single number (scalar)        
        
        2 arrays of the same shape
            a + b                   -> array([10, 12, 14, 16])  / arithmetic operations with same shape array

        2 arrays of different but compatible shape
            a = np.array([[1, 2, 3, 4], 
                          [5, 6, 7, 8], 
                          [9, 1, 2, 3]])
            b = np.array([4, 5, 6, 7])
            a + b                   -> array([[ 5,  7,  9, 11],
                                              [ 9, 11, 13, 15],
                                              [13,  6,  8, 10]])
            

    Boolean arrays
        - arrays of True and False created dynamically from boolean conditions

        |       -> or
        &       -> and
        ~       -> not (tlde)
        ==
        !=
        > <

        a = array([1, 2, 3, 4, 5])
        a > 4                       -> array([False, Fale, False, True, True])
        a[a > 4]                    -> array([4, 5]) 
        np.where(condition)         -> return indexes of the condition  

    main functions and methods (called routines) [all of these are ok both as method and as function]

        slicing
            [::]
            note: slicing access the same memory object as the original array
            thus if you change a number in one of the original arrays you will also change it in the sliced portion
            to be completely sure of separating stuff use .copy()
            a[:]                                        -> slice 1 dimension. rows 
            a[:, :]                                     -> slice 2 dimensions. rows and columns
            a[::-1]                                     -> reverse the order of the array
 
        
        Mathematics and statistics
            np.sum()                                     -> return the sum of the array
            np.mean()                                    -> return the median of the array
            np.std()                                     -> return the standard deviation
            np.var()                                     -> return the variance
            np.exp()
            np.round()

        Linear / Matrix algebra
            np.dot(array1, array2)                      -> return the matrix product: sum of (a1[0] * a2[0], a1[1] * a2[1]....)
            a.dot(b)                                    -> return the matrix product
            @                                           -> return the matrix product
            np.matmul()                                 -> retun the matrix product
            *                                           -> return element wise multiplication


        Array creation and manipulation                                             
            np.arange(start, end, step)                  -> similar to range in python. it creates an array with values from start to end(not included)
            np.zeros(size)                               -> return an array of 'size' filled with 0s
            np.ones(size)                                -> return an array of 'size' filled with 1s
            np.full(size, value)                         -> return an array of size filled with value / np.full((2,2), 22)
            np.empty(sze)                                -> return an array of 'size' empty
            np.linepace(start, stop, size)               -> return an array of size, from start to end with equally space numbers
            np.pad()                                     -> pad ana array

            np.zeros_like(array)                         -> Return an array of zeros with the same shape and type as a given array.
            np.ones_like(array)                          -> Return an array of ones with the same shape and type as a given array.
            np.full_like(array)                          -> Return an array of ones with the same shape and type as a given array. 
            np.eye(size)                                 -> Return a 2-D array with ones on the diagonal and zeros elsewhere.
            np.identity(size)                            -> Return the identity array. The identity array is a square array with ones on the main diagonal.
            np.copy(array)                               -> make a copy of an array

            np.all(condition)                            -> Test whether all array elements along a given axis evaluate to True.
            np.any(condition)                            -> Test whether any array elements along a given axis evaluate to True.
            np.argwhere(condition)                       -> return the index of an arrya based on a condition (es. a == 6)

            np.sort()                                    -> sort the array
            np.hstack()                                  -> stack arrays horizontally
            np.vstack()                                  -> stack arrays vertically
            np.concatenate()                             -> concatenate 2 arrays
            np.reshape(new_form) / a.reshape(new_form)   -> return a reshaped array without changing the original 
            np.resize(newform) / a.resize(new_form)      -> reshape the orginal array 
            np.newaxis                                   -> add a new dimension. eg a = a[np.newaxis, :] or a[:, np.newaxis]
            np.expand_dim(a, axis=)                      -> add a new dimension at the specified location. b = np.expand_dims(a, axis=1)
            np.transpose() / a.transpose()               -> reverse the axis of an array
            np.flip() / a.flip()                         -> reverse the content of an array like ::-1 but with the agility of choosing the axis
            np.flatten() / np.ravel()                    -> flatten ana array. flatten create a copy while ravel creates a view

            += -= *= ...                                 -> operations that modify the existing array instead of creating a new one

        random are only functions
            np.random.randint(start,end, size)           -> create and array of 'size', filling it with randon INT nr from start, end
            np.random.rand(size)                         -> return a random array of size', filled with float nrs form 0 to 1 / normal distribution 
            np.random.randn((size))                      -> return a random array of size', filled with float nrs form 0 to 1 / bell curve
            np.random.permutation(np.array)              -> shuffles the array
            np.random.sample(array)                      -> create and array with random nr from 0 to 1 of the same shape of the argument 

3+) NUMPY VECTORIZATION  

    Code 
        np.dot(vector, vector)
        np.matmul(matrix_1, matrix_2)
        AT or A_in = A.T --> to transpose
        


    Dot product (vector x vector multiplication)
        a = [1, 2]
        w = [3, 4]
        z = np.dot(a, b) = (1x3) + (2x4) = 11

    Vector x matrix multiplication
        a = [1, 2]
        W = [[3, 4],
             [5, 6]]
        z = at x W = [ (1x3 + 2x4) (1x5 + 2x6)] = [11, 17]
        z = np.dot(a, W)
        z = NP.matmul(a, W)

    Matrix x Matrix multiplication
        a = [[1, 2],
             [-1, -2]]
        W = [[3, 4],
             [5, 6]]
        z = at x W = [ [(1x3 + 2x4) (1x5 + 2x6)] ] 
                       [(-1x3 + -2x4) (-1x5 + -2x6)] ]

        z = [[11, 17], [-11, -17]]
        z = np.dot(a, W)
        z = NP.matmul(a, W)

    MXM MUltiplications Rules and steps
        - Transpose the first matrix A. and considers the rows as vectors when you are dealing with a transpose
        - take the second matrix W and considers the columns as vectors when you are dealing with a matrix
        - the new matrix Z is big as A_rows x W_columns
        - A_columns and W_rows should match

    example 

        A = [1, -1, 0.1]
            [2, -2, 0.2]
            -> 2x3 Matrix
        
        AT = [1, 2]
             [-1, -2]
             [0.1, 0.2]
             -> 3x2 Matrix

        W = [3, 5, 7, 9]
            [4, 6, 8, 0]
            -> 2 x 4

        AT_rows = 3
        AT_columns = 2

        W_rows = 2
        W_columns = 4

        AT_C and W_columns match

        AT x W = [x, x, x, x]
                 [x, x, x, x]
                 [x, x, x, x]
                 -> 3X4 Matrix

4) PANDAS
 
    import pandas
    import numpy

    Panda Series
        - a concept of ordered, indexed sequence of elements
        - very similar to a python list / numpy array but with lot of differences
        - a series has an associated data type: eg. float64
        - the underlying structure is numpy array
        - a series can have a name
        - a series has a type
        - a series has values
        - a series has an index, similar to a python list
        - in contrast to lists, with sequences we can explicitely set and change the indexes
            - by default index start from 0
            - but arbitrarely you can set them differently
        - it now looks a little bit more like a dictionary, it is almost an hybrid between a dict and a list
        - but differently from dicts, series are always ordered. they are more like a single column of a table.

        summary: INDEXED, ORDERED, HAS A DTYPE, A NAME, VALUES

        .Series()                 -> create an empty data series
        .Series(X)                -> convert X into a data series (x can be a list)
        .sort_values()            -> sort the series values
        .name                     -> set the name of the series
        .dtype                    -> set the type of the series
        .values                   -> set the values of the series
        .index                    -> set the index of the series

            eg:
            g7_pop = pandas.Series([1, 4, 6, 7, 8, 9])
            g7_pop.name = 'g7 population'
            g7_pop.dtype = int
            g7_pop.values = array([1, 4, 6, 7, 8, 9])
            g7_pop.index = [italy, france, germany...] or [0, 1, 2, 3...]

        Calling the elements and multiindexing
            data['indexname']               -> by name -> g7_pop[italy] = 1
            data.iloc['position']           -> by index -> g7_pop.iloc[0] = 1
            data['idxname1','idxname2'...]  -> g7_pop['italy', 'france'] = 1, 4 / multiindexing with idx names
            data.iloc[pos1, pos2]           -> g7_pop.iloc[0, 1] = 1, 4 / multiindexing with positions

        Slicing
            - in pithon list, the end of the slice is excluded / in panda series the upper limit is included
            data[::]                        -> g7_pop = pandas.series([1, 4, 6, 7, 8, 9]) / g7_pop[:3] = 1, 4, 6, 7

        operations, methods and boolean operations
            - just like numpy arrays
            < > == 
            .sum() .mean() .std() etc...
            | & tilde

        types
            dtype=                          -> to set the type as a parameter
            s.astype / df.astype            -> to change an existing data as a parameter

    Panda DataFrame

        axis = 0 is the row.  
        axis = 1 is the columns. 

        - it is basically built as an xls table with rows and columns (matrix)
        - at the fundamental level a column of a data frame is a panda Series
        - in fact, a data frame is just a combination of series with the same indexes
        - see tje df as a dictionary of lists /  series

        pd.DataFrame(data = xxxx)       --> create a dataframe from data
        pd.to_frame()                   --> frame stuff
        pd.to_dict()                    --> convert DF into a dictionary (a list of dictionaries) 
    
        main info
            .columns                    --> its the name of the series
            .index                      --> its the index of the series
            .info()                       --> gives you info about the structure of the data frame
            .size                       --> size of the df
            .describe()                   --> a summary of the statistics of the df (min, max, mean...)
            .dtypes                     --> gives the dtype each column has
            .shape                      --> return the shape of the df

        indexing, selection, slicing, filtering
            df.loc['rowname']           --> select a row by name / also slicing
            df.iloc[-1]                 --> select a row by index / also slicing 
            df['columnname']            --> select a column / also slicing
            df.at[row, column]          --> select a specific cell via labels
            df.iat[3,0]                 --> select a specific cell via indexing         
            df['column']['row']         --> select a specific cell
            df.head(n)                  --> return the first n rows
            df.tail(n)                  --> return the last n rows
            df.nlargest()               --> sort an return the n largest elements 
            df.nsmallest()              --> sort and return the n smallest elements
            df.sample(n)                --> return a sample of n rows
            df.isin()                   --> return only the rows that are in the destination

            --> the same methods can be used for slicing
            df.loc['italy':'france']
            df.loc['italy':'france'['population','gdp']]  --> we can also apply a second dimension on the columns


        multiindexing
            MultiIndex.from_arrays()
            MultiIndex.from_tuples()
            MultiIndex.from_product()
            MultiIndex.from_frame()



        conditional selection
            df['population'] > 70                         --> just like series, it filter the df based on the condition       
            new_df = df[df['population'] > 100000000]     --> filter rows directly
            new_df = df.loc[df['population'] > 100000000] --> filter rows with loc

            df.any()                                      --> Test whether any array elements along a given axis evaluate to True.
            df.all()                                      --> Test whether all array elements along a given axis evaluate to True.
            df.where()                                    --> return a df of the same shape of the original but only with value respecting the condition, the rest is nan

        REMEMBER: all operations on df and series are IMMUTABLE. the original data structure is not changed
        inplace = True --> UNLESS you use the parameter 


        modifying data frames
        
        adding columns and deleting columns works like a dictionary
            df['language'] = lang                          --> adding a new column
            df['gdp per capita'] = df['gdp'] / df['pop']   --> creating columns from other columns
            df.insert(location, 'columnname', values)      --> adding a new column at a specified location
            del df['column']                               --> delete columns
            df.pop('column')                               --> delete columns
            df.drop('canada')                              --> delete column or row
            df['language'] = english                       --> change elements in the columns
            df.loc['row', 'column'] =                      --> modify the cell

            df.columns = ['column1', column2]              --> set the header of the columns
            df.rename(columns={}, index={})                --> rename columns and / or index
        
        sort, merge, group, transpose
            df.sort_values()                               --> sort values in a column
            df.sort_index()                                --> sort the index
            
            df.pivot_table()                               --> basically creates a pivot
            df.stack() / df.unstack()                      --> pivot e depivot on hierarchical indexes
            df.groupby('rowvalue')[columns].func()         --> aggregation of column data based on a row parameter
            df.transpose() / df.T                          --> transpose the df  
            
            df.merge(other_df, on='pivot_column')          --> merge the second df into the first df leveraging a common pivot column
            pd.concat([frames])                            --> concatenate 2 or more data frames (like append)     

        apply functions
            df.pipe(func)                                  --> apply func on entire df   
            df.apply(func)                                 --> apply func on a column or row
            df.applymap(func)                              --> apply func elementwise

        math and statistics
            .min() / .idxmin()
            .max() / .idxmax()
            .mean()
            .sum()
            .cumsum() - cumulative sum
            .sumprod() - cumulative product
            .std()
            .median()
            .var()
            .value_counts()

    Data cleaning

        4 steps process
            1. Handling Missing Data
            2. Handling Invalid Values (str instead of int) 
                + Handling domain problems (age of 170. a valid value, but out of the limit of its domain)
            3. Handling duplicates
            4. Handling non numerical values
            5. Data visualization

        1. Handling Missing Data

            NaN

            info to remember
                np.nan != np.nan
                None = np.nan
                nan is a float
                it is Nat for datetime missing values
            
            math operations with nan 
                When summing data, NA (missing) values will be treated as zero.
                If the data are all NA, the result will be 0.
                Cumulative methods like cumsum() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.
                NA groups in GroupBy are automatically excluded.

            Pandas utility functions to detect Null values

                pd.isna()
                pd.notna()

                they are all synonyms
                pd.isnull(np.nan)
                pd.isnull(None)
                pd.isna(np.nan)
                
                the opposite versions
                pd.notnull(np.nan)
                pd.notnull(None)
                pd.notna(np.nan)
                pd.notna(None)

            Counting and identifying Nan

                Series
                    s.isnull().sum()                    -> Sum of null values / actually the count of null values
                                                        -> why sum(): in series 1 are True and 0 are Zeros, so the sum return the count of Falses
                    s[pd.notnull(s)]                    -> filters all the values in a series that are not null

                Data frames
                    df.info()                           -> to understand where and how many null values are in the data frame
                    df.isnull.sum()                     -> work the same way as the s.isnull().sum()
               
            Fixing / filling Nan Values: both for series and data frames
                s/df.fillna(value)                      -> fill the nan values with 'value'. eg. s.fillna(0), fill it with 0
                s/df.fillna(method='ffill')             -> forward fill. the previous values is used to fill the nan
                s/df.fillna(method='bfill')             -> backword fill, the next valid value is used to fill the nan
                    with df just add axis = 
                s/df.replace()                          -> to replace values with nan / the inverse of fillna

            dropping Nan values
                s/df.dropna(axis=0)                     -> drop Entirely any rows that as Any null value (at least a null value)
                s/df.dropna(axis=1)                     -> same drop but on columns
                s/df.dropna(....)                       -> applly conditions to the drop. eg drop only rows that have more than 3 Nan values
                    df.dropna(how='any')                -> default behaviour
                    df.dropna(how='all')                -> drop rows/columns that have all values Nan
                    df.dropna(thresh=3)                 -> set threshold to 3. drop everything that has more than 3 nan values

        2. Handling Invalid Values

            df['column'].unique()                           -> return the unique values of the column
            df['column'].value_counts()                     -> return a value count of the column to see if there are invalid values
            df['column'].replace({'newvalue':'oldvalue'})
            Handling domain problems --> just be good at coding and problem solving

        3. Handling duplicates

            .unique()                                       -> return the unique values of the column
            .isunique()                                     -> return boolean if values are unique

            in series
            s.duplicated()                                  > return a True False series considering value duplicates from top to down
            s.duplicated(last)                              -> in reverse order: bottom up 
            s.duplicated(keep=False)                        -> all duplicated values are set to False
            s.drop_duplicates()                             -> keep only the non duplicates
                work also with (last) and (keep=False)

            in data frames
            df.duplicated(subset=['column'])                -> same as series but specify the column
            df.duplicated(subset=['column'], keep=last)
            df.drop_duplicated(subset=['column'])
            index.duplicated()

        4. Handling String / datatimes / categorical

            in pandas, all types of values, except for numerical have category methods
            more specifically
                .str    -> for strings
                .dt     -> for date times
                .cat    -> for categorica

            for strings (very similar to classic python)
                .str.split()                        -> return a series of list 
                .str.split(..., expand=True)        -> return a data frame from it 
                .str.rsplit()                       -> reverse split                  
                .str.contains()
                .str.strip() / .str.lstrip() / .str.rstrip()
                .str.replace()
                .str.upper()
                .str.lower()
                .str.capitalize()
                .str.casefold()
                .str.len()
                .str.cat() -> for concatenation
                .str.join()                         -> Join strings in each element of the Series with passed separator
                .str.startswith()
                .str.endswith()
                .str.find()
                .str.findall()
                .str.extract()                      -> call re.search on each element, returning DataFrame with one row for each element and one column for each regex capture group
                .str.extractall()                   -> Call re.findall on each element, returning DataFrame with one row for each match and one column for each regex capture group
                isdigit() / str.isdigit
                isspace() / str.isspace
                islower() / str.islower
                isupper() / str.isupper
                istitle() / str.istitle
                isnumeric() / str.isnumeric
                isdecimal() /str.isdecimal

                these methods can be combined with regular expressions

            for datetime
                pd.to_datetime(element)                 -> change the element into date format
                pd.DatetimeIndex                        -> extract specific date components from a datetime value
                                                            df['year'] = pd.DatetimeIndex(covid_df.date).year
                                                            df['month'] = pd.DatetimeIndex(covid_df.date).month
                                                            df['day'] = pd.DatetimeIndex(covid_df.date).day
                                                            df['weekday'] = pd.DatetimeIndex(covid_df.date).weekday

    Reading Data

        Reader              ->      Writer 
        read_csv                    to_csv   // also good for txt
        read_json                   to_json
        read_html                   to_html
        read_excel                  to_excel
        read_sql                    to_sql

        1. reading csv methods and paramethers

            pd.read_csv?        -> ? is to discover info on a function  
            
            pandas can also read remotely from url. just set the url as the file
                csv_url = '.....'       -> set the url
                pd.read_csv(csv_url)    

            header:             -> set the header. set as the first row as the default
            names:              -> Sequence containing the names of the columns (used together with header = None).
            index_col:          -> Index of the column or sequence of indexes that should be used as index of rows of the data.
            dtype:              -> Dictionary in which the keys will be column names and the values will be types of NumPy to which their content must be converted.
            sep:                -> Character(s) that are used as a field separator in the file.
            filepath:           -> Path of the file to be read.
            skiprows:           -> Number of rows or sequence of row indexes to ignore in the load.
            na_values:          -> Sequence of values that, if found in the file, should be treated as NaN.
            parse_dates:        -> Flag that indicates if Python should try to parse data with a format similar to dates as dates. You can enter a list of column names that must be joined for the parsing as a date.
            date_parser:        -> Function to use to try to parse dates.
            nrows:              -> Number of rows to read from the beginning of the file.
            skip_footer:        -> Number of rows to ignore at the end of the file.
            encoding:           -> Encoding to be expected from the file read.
            squeeze:            -> Flag that indicates that if the data read only contains one column the result is a Series instead of a DataFrame.
            thousands:          -> Character to use to detect the thousands separator.
            decimal:            -> Character to use to detect the decimal separator.
            skip_blank_lines:   -> Flag that indicates whether blank lines should be ignored.

        2. reading from databases methods

            import sqlite3              -> there can be different libraries depending on the db you are using. the apis are the same

            vanilla Python way
                conn = sqlite3.connect('dbname')                    -> creates a connection to the db and store it into an object
                cur = conn.cursor()                                 -> create a cursor. A cursor allows us to execute sql queries
                cur.execute(wrote query here)                       -> execute query
                results = cur.fetchall()                            -> fetch all query results in a variable (a list of tuples)
                df = pd.DataFrame(results)                          -> now make a data frame out of it by adding pandas to the equation
                remember to close the cursor and connection
                cur.close()
                conn.close()
            
            with pandas

                conn = sqlite3.connect('dbname')
                df = pd.read_sql('query', conn)                     -> directly transform it into a df from the connection with read_sql
                read_sql is a more general function of 2 more specific functions. it is a wrapper of
                    read_sql_query                                  -> the default behaviour
                    read_sql_table                                  -> read an entire table totally
                        with _table you need
                            - import sqlalchemy                     -> import this module
                            - create_engine('db url')               -> create and engine
                            - con = engine.connect()                -> connect to the engine
                            - df = pd.read_sql_table('employees', con = connection)
 
        3. reading from html

            html_string = 'url'                 -> download the content
            dfs = pd.read_html(html_string)     -> html can have multiple tables
            len(dfs)                            -> know how many tables there are
            pd.read_html(html_string)[0]        -> to read only the first table

5) MATPLOTLIB and SEABORN

    import matplotlib.pyplot as plt
    import seaborn as sns
    %matplotlib inline                  -> ensure that charts are embedded within the notebook / idem

        Line Plot (matplotlib)

            Plotting the chart
                plt.plot(data)              -> return a line chart and the object in memory  
                plt.plot(data);             -> return just the chart
            
            Plotting multiple lines
                just put 2 plt.plot(data) in the same notebook form
        
            Customizing the X_axis and chart
                x_axis = [stuff,,]          -> set values for x axis in a list
                plt.plot(x_axis, data)      -> return the chart but using custom x_axis and y_axis
                plt.xlabel                  -> give a name to the x axis
                plt.ylabel                  -> give a name to the y axis 
                plt.title                   -> give a title to the chart 
                plt.figure(figsize=(12,6))  -> change the figure size

            Styling arguments  https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html
                format [market][line][color]
                marker = 'o'                -> marker argument to give form to the lines. all markers here https://matplotlib.org/3.1.1/api/markers_api.html
                color or c                  -> Set the color of the line (supported colors) https://matplotlib.org/3.1.0/gallery/color/named_colors.html
                linestyle or ls             -> Choose between a solid or dashed line
                linewidth or lw             -> Set the width of a line
                markersize or ms            -> Set the size of markers
                markeredgecolor or mec      -> Set the edge color for markers
                markeredgewidth or mew      -> Set the edge width for markers
                markerfacecolor or mfc      -> Set the fill color for markers
                alpha                       -> Opacity of the plot

                You can also edit default styles directly by modifying the matplotlib.rcParams dictionary.
                Learn more: https://matplotlib.org/3.2.1/tutorials/introductory/customizing.html#matplotlib-rcparams .
                    matplotlib.rcParams['font.size'] = 14
                    matplotlib.rcParams['figure.figsize'] = (9, 5)
                    matplotlib.rcParams['figure.facecolor'] = '#00000000'

            Seaborn default styles
                sns.set_style("whitegrid")
                sns.set_style("darkgrid")
                              
        Scatter Plot (seaborn)

            Plotting a scatter plot 
                sns.scatterplot(x=data , y=data );           -> return a scatterplot
                sns.scatterplot(x=column, y=column, data=df)
                arguments
                    hue = differnet colors                   -> separate dots into different colors          
                    s =  size                                -> size of the dot
        
        Histogram (matplotlib)

            Plotting an histogram
                plt.hist(data)                              -> return an histogram

            Plotting multiple histograms
                just put 2 plt.hist(data) in the same notebook form
                    set alpha = to control overlapping opacity

                    arguments
                        bins = nr                           -> set the nr ans size of bins

        Bar charts (matplotlib)

            Plotting a bar chart
                plt.bar

                arguments   
                    bottom=             -> Stacking multiple bars on top of one another
            
            Plotting a bar chart with averages
                sns.barplot(x=, y=, data= data, hue=)
                (you can make it horizontal by switching axes)
                            
        Heatmap (seaborn)

            sns.heatmap(dataframe);
                annot=True                  -> to display also the actual values
                cmap=                       -> change the color palette

        Other plots in seaborn (contour plot, )

            sns.kdeplot()
            sns.boxplot

        Subplots 

            fig , axes = plt.subplots(nr_rows,nr_colums, fisize=())    -> return a set of axes for Plotting
            
            axes[row, column].plot(data)                               -> create the plot for the first row, first column position [0,0]
                
            sns.pairplot                                               -> to automatically plot several different charts for pairs of features within a dataframe.

        Initial configuration

            sns.set_style("darkgrid")
            matplotlib.rcParams['font.size'] = 14
            matplotlib.rcParams['figure.figsize'] = (9, 5)
            matplotlib.rcParams['figure.facecolor'] = '#00000000'



E - MACHINE LEARNING & AI
-------------------------------------

0) FOUNDATION OF AI PRODUCT BUILDING AND COMPANY BUILDING

    - AI > Artificial Narrow Intelligence (subset of AI) > Machine Learning (subset of ANI) > Deep Learning (called neural networks, subset of ML) 
    - Data Science is another circle and at the ntersection of ML and AI
    - be careful of fake ai and buzzwords

    - Machine Learning 

        - Supervervised Learning: input A --> Output B mapping
        - what can it do: automating simple task that humans can do in 1 second
        - what it cannot do: reasoning and complex tasks, automate from small dataset, understand complex assignments and descriptions

        - Unsupervised Learning: clustering data
        - transfer learning: learn from task A, and use knowledge to help on task B 
        - Reinforcement learning: use a reward signal to tell AI when it is doing well or poorly
        - GANs: generative adversarial network. Synthesises new images from scratch 
        - Knowldedge graphs 
    
    - machine learning vs classical programming

        - classical programming: input DATA AND RULES --> COMPUTE CODE --> OUTPUT ANSWERS
        - machine learning: input DATA AND ANSWERS --> ML MODELS --> OUTPUT RULES
    
    - general inputs for good ML:
        - More and valuable data
        - Large scale neural networks
    
    - types of data: 
        - structured (spreadsheet and numbers)
        - unstructured data (images, audio, text)

    - What makes and AI company / strategic function for an AI centric company  
        - Strategic data acquisition (the raw material of all AI products)
        - Unified data warehouse (the organizations, operations and logistics of raw material, semiproducts and fina products)
        - Pervasive automation products (Ai products are very often automation products)

    - AI startup / Automation path
        - Execute pilot projects to gain momentum
        - Build an AI / co-founding team
        - Provide broad AI training 
        - Develop an AI product, strategy and Vision
        - Market it

        - build network effect and competitive advantage on data
            - the virtuous cycle of ai: buld product -> more user -> more valuable data -> better product -> more user -> more valuable data .... 

    - birth of an ai product 
        - there's a problem
        - make hypothesis on how to fix it 
        - develop simple heuristics (how to fix it easily and prove the market)
        - measure impact
        - build a more complex and automated ml model
        - measure impact
        - tune the model
        - replace existing technique
        - iterate

    - Workflow of an AI projects
        - Collect data and clean/prepare data
        - Train the model, test and iterate towards optimization until good enough
        - Deploy the model
        - get new data back
        - repeat and improve

    - Selecting AI projects
        - Intersection of what AI can do (state of the art doability) and what is valuable to the market, biz or consumers (practicality of business, utility)
        - therefore, tech expertise and business expertize unified + ethical responsability

        - think automating tasks instead of jobs nd entire functions
        - what are the main drivers of business value
        - what are the main pain points to solve

    - technical diligence
        - product effectiveness: can the AI system work / can we build it
        - strategic data acquisition: how much data is needed / can we get the data / how do we get the data
        - engineering time and resources: how much time, resources, people are needed / is it agile
    
    - business diligence
        - does it increase revenue
        - does it lower costs
        - does it open new business opportunities (launch new products and businesses)
        - are we reinventing the wheel? trying to run in front of a train? some things are industry standard, use them and avoid re-building them
    
    - possible ai problems to fix 
        - current limitation of ML
            - performance limitations
            - explainability is hard
            - ethical and economic issues such as adversarial attackes and biased ai
        - insufficient data
        - invalid / mislabled data
        - ambiguous data

    - technical tools
        - AI open source frameworks such as tensorflow, pytorch, ...
        - Arxiv for the latest research
        - Github for pieces of software 
        - CPU and GPU 
        - Cloud vs on-premise vs edge computing
    
    - Roles in AI team
        - software engineer
        - machine learning engineers - build ai systems and products 
        - machine learning researcher - extend state of the art in machine learning 
        - applied machine scientist - research and engineering combined ]
        - data engineer
        - AI product manager (what to build, product-market fit, tech doability, business value,...)

    - common pitfalls in ai, dos and datapoints
        - be realistic about what ai can do / don't think ai can solve everything 
        - build tech and buz teams together / don't just rely on engineers
        - think iteratively / it never works the first time 
        - a small hardworking team is good to start / don't think you need superstars engineers

    - Major applications for supervised learning (most economically impactfull method right now)

        - Computer vision
            - image classification / object recognition 
            - object detection 
            - image segmentation 
            - tracking movement 
        
        - Natural Language Processing 
            - text classification 
            - information retrieval
            - machine translation 
            - name entity recognition 
            - parsing and speech tagging

        - Audio and speech
            - Speech to text 
            - Text to speech
            - speaker identification 
            - trigger word, wakeword detection

        - Robotics
            - perception: figuring out what's in the world around you  
            - motion planning: finding a path for the robot to follow 
            - control: sending commands to the robot to follow a path 
        
    - ethical issues with ai
        - biased ai through biased data
        - adversarial attackes on ai and zero sum games 
        - adverse ai
        - ai and job elimination 

1) ML DEFINITIONS AND STEP-BY-STEP APPROACH

    What is machine learning: Field of study that gives computers the ability to learn without being explicitly programmed
    
    1) supervised learning: input x --> output y mapping / learn from data labeled with the right answer
                                                      / The goal is usually predict data or attach labels to future data

        regression: predict a number / infinitely many possible outputs / usually numerical, structured data 
        classification: predict a category / small number of possible outputs / usually text-images-sound, unstructured data 

    2) unsupervised learning: find something interesting in unlabeled data / the algo has to find structure and patterns in the data 
        clustering: group similar data points together 
        anomaly detection: find unusual data points 
        dimensionality reduction: compress data using fewer numbers 

    3) Step by step approach

        Ful Cycle of ML projects

            1. Define the scope, goal, problem-solution
            2. Strategically collect data 
            3. Build, train, error analysis and iteratevely improve the model
            4. Deploy, monitor and mantain the system (MLOps) -> functioning, scaling, logging, model updates, monitoring 
            Iteratevely optimize, scale and upgrade 


        STEP 1: PREPARE DATA 
                - retrieve input and output data in numpy arrays
                - study the data: shape/len, type, scatter plots, stats...
                - normalize / features scale all parameters
                - apply feature engineering if necessary and useful

        STEP 2: DESIGN THE MODEL
                - set an initial w and b parameters to 0 and 0
                - specify the model/function (linear, logistic..), aka how to compute output given input x and parameters w,b
                - calculate the initial prediction y-hat (wx + b) using the model

        STEP 3: COMPILE AND TRAIN THE MODEL
                - calculate the loss and cost function j(w,b) = delta between y-hat and y for each set of input
                - choose initial alpha very low as 0,01 and compute initial gradient of w and b
                - apply gradient descent algorithm by updating w, b iteratively and gradually reduce the cost function j(w,b)
                - apply it for a set diffent nr of iterations to check. from 10k to 100k
                - check cost function is gradually going down through a linear chart  

        STEP 4:  DIAGNOSTIC, VALIDATION AND OPTIMIZATION OF THE MODEL
                - choose the optima model and parameters using train, dev and test data sets [calculate bias / variance]
                - choose the right lambda value for regularization 
                - valuate the model on the test set
                - apply error analysis to improve

                wht to try nect to debugging
                    - fixing high variance:
                        - get more training examples 
                        - try a smaller set of features aka simplify your model 
                        - decreasing lambda for regulaization aka simplify yoyr model 

                    - fixing high bias
                        - get additiona lfeatures aka get more flexibility to your model 
                        - add polynomial features aka get more flexibility to your model  
                        - increase lambda for regularization aka get more flexibility to your model 

                ITERATE AND REPEAT TO PERFECT AUTOMATION

        STEP 5: PREDICT NEW DATA WITH FORWARD PROPAGATION 
                - once found the final model and values w and b and lambda that minimize the cost function J(w,b) 
                - apply the model to new unlabeled data to predict the output y. 

--- Supervised Learning ---

2) LINEAR REGRESSION

        Terminology
            x                       -> input variable / feature variable 
            y                       -> output variable /  target variable 
            m                       -> number of training examples 
            (x,y)                   -> ingle training example 
            (x_i, y_i)              -> i th trainign example 
            f or f(x) or f(w,b)     -> the function / model               
            y-hat                   -> output prediction, the estimated y
            w, b                    -> parameters
            w                       -> the slope, the inclination of the linear regression
            b                       -> the interception of the y of the linear regression

        Linear Regression model with 1 variable / Univariate linear regression 
            f(x) = wx + b
            y-hat = wx + b   

        Linear Regression model with multiple variables 
            f(x) = no.dot(w, x) + b         -> where w and x are vectors
            y-hat = np.dot(w, x) + b

            we use vectorization to calculate the preduction 
            vectorization is faster because numpy arrays use parallel computing   
            basically we calculate the dot product of vector x and vector W
            b is a scalar                                                     
 
        Cost function to minimize 
            j(w,b) = 1 / 2m * sum (y_hat - y) ** 2
            j(w,b) = 1/ 2m * sum (wx + b - y) ** 2

        Cost function to minimize with multiple parameters 
            j(w,b) = 1 / 2m * sum (y-hat - y)
            j(w,b) = 1 / 2m * sum (np.dot(w, x) + b - y)    -> where w, x are vectors

            using vectorization on each elements of the input set from 1 to n
            we calculate the cost function for each row of input sets from 1 to m 
        
        gradient descent algorithm
            alpha                   -> learning rate. a small number between 0 and 1. usually 0.01, it represents the size of the gradient descent step
            derivative of j wrt w   -> the direction toward which the gradient descent will take the step wrt w 
            derivative of j wrt b   -> the direction toward which the gradient descent will take the steo wrt b 

            an algorithm that change w and b gradually until the cost functin J(w,b) is minimized 
            it is an iterative algorithm repeated until convergence
            w and b must be changed simultaneously

            w = w - alpha * ( 1/m * ( sum( wx + b - y) * x ) )
            b = b - alpha * ( 1/m * ( sum( wx + b - y) ) 
            = as assignment like in coding (non math truth value)

            if derivative of w or b are positive -> w and b will decrease 
            if derivative of w or b are negative -> w and b will increase 

        gradient descent algorithm with multiple variables
            w = w1 - alpha * ( 1/m * ( sum( wx + b - y) * x1 ) )      -> where w and x are vectors from 1 to n
            b = b - alpha * ( 1/m * ( sum( wx + b - y) ) 
            = as assignment like in coding (non math truth value)

            we calculate the delta y-hat vs y (wx + b - y) where w and x are vectors with elements from 1 to N
            for each element x1, w1 from 1 to n we apply gradient descent with parallel computing simoultaneously
            we do it for each row of inputs set from 1 to m

        Feature Scaling
            parameters could be too large and different between them to apply gradient descent, we need to rescale them
            the ideal range is between -1 and 1 or similar

            Dividing by max input       ->      x1 scaled = x1 / max(x)
            Mean normalization          ->      x1 scaled = (x1 - mean(x)) / (max(x) - min(x))
            Z-score normalization       ->      x1 scaled = (x1 - mean(x)) / std(x)

        Choosing alpha
            alpha should allow gradient descent but it should not be too slow
            values to try: 0.001 / 0.01 / 0.1 / 1
            approach
                - start with 0.001
                - multiply it by 3
                - try 0.01
                - multiply it by 3
                ....

        Feature engineering 
            - modeling and engineering on input data to generate new parameters that better relate to the prediction
            - example: input: lenght of the gradient + width of the gradient
                       a better parameter would be the size of the garden (lenght * width) 

3) LOGISTIC REGRESSION

        Terminology

            Logistic regression     -> sigmoid function
            y == 1                  -> positive class, presence / true value in binary classification
            y == 0                  -> negative class, absence  / false value in binary classification

        Logistic function (sigmoid function)
            f(x) = wx + b
            z = wx * b
            g(z) = 1 / (1 + e** -z)
            where 0 < g(<) < 1              -> probability that a class i 1. eg 0,7. (the inverse is the probability that a class is 0)
                if z is a large positive number -> g(z) will be closer to 1
                if z is a large negative number -> g(z) will be closer to 0

            in code
                g = 1 / (1 + np.exp(-z))

        Decision boundary
            set decision boundary at
            z = 0
            wx + b = 0

        logistic regression cost function
            𝑙𝑜𝑠𝑠 = −𝑦*log(𝑓(𝐱))−(1−𝑦)*log(1−𝑓(𝐱))        -> loss for a single training example, where f(x) is the sigmoid function
            J(w,b) = 1/m * sum(losses) 

        logistic regression gradient descent
            w = w - alpha * ( 1/m * ( sum( f(x) - y) * x ) )
            b = b - alpha * ( 1/m * ( sum( f(x) - y) ) 

4) NEURAL NETWORKS

    Architecture of a neural network
        - how many layers to build
        - how many neurons in each layers

    import tensorflow as tf 
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Normalization
    from tensorflow.keras.activations import linear, relu, sigmoid, softmax
    from tensorflow.keras.losses import BinaryCrossentrop, MeanSquaredError, SparseCategoricalCrossentropy

    data in tensorflow
        tensorflow uses matrixes
        the result of functions such as Dense is a datatype called Tensor, very similar to a matrix

        a1.numpy() -> to transform a Tensor into a numpy matrix
        a1.tensor() -> to transform a numpy matrix into a Tnesor

    Normalize Data 
        norm_l = Normalization(axis=1)                                  -> create a normalization layer
        norm_l.adapt(X)                                                 -> adapt the data and learn mean and variance
        Xn = norm_l(X)                                                  -> normalize the data
        
    Step 1 - specify the neural network model (linear or logistic regression)
        
        Dense()                                                         -> single layer model
        Sequential()                                                    -> multi-layers model

        Single layer / Dense()
            x = np.array([[xxx, xxx]])                                      -> input values stored as a numpy matrix (not array)
            layer_1 = Dense( units = 3, activation = 'linear' / 'sigmoid')  -> apply linear / logistic regression to a single layer
                unit = 3                                                    -> specify the number of neurons in the layer
                activation =                                                -> specify the machine learning function to use linear / sigmoid / RELU / softmax
                name =                                                      -> give a name to the layer
                kernel_regularizer =                                        -> to apply regularization with L2(lambda)
            a1 = layer_1(x)                                                 -> store output of layer 1 in a1

        Multi-layers / Sequential()
            model = Sequential ([
                Dense(units=3, activation='sigmoid'),
                Dense(units=1, activation='sigmoid')]) 

        Most used Activation Functions [Linear, Logistic, ReLU]
            g(z) = wx + b   //  Linear activation function
            g(z) = 1 / (1+e**-(wx + b))     //  Logistic activation function
            g(z) = max(0, z)    //  max(0, wx + b)      //  ReLU Rectified linear unit  

            How to choose activation functions

                For Output Layer
                - Start from the type of output you want. The type of output determines what activation function is best suited for the model
                - For binary classification problems                                ---> Logistic functions
                - For regression problems                                           ---> Linear function
                - If Y is a regression problem but can only take on positive values ---> ReLU
                - for multiclass classification problem                             ---> softmax activation

                For Hidden Layers
                - the ReLu is the most widely used activation function for hidden layers. Why?
                    - faster in learning than sigmoid. Relu is flat in only 1 part of the grapth (sigmoid is flat in 2 parts of the graph), therefore gradient descent is slower.
                                                                        
    Step 2 - Compile and train the neural networks model

        model.compile(optimizer = , loss=)                              -> defines a loss function and specifies a compile optimization.
            loss = BinaryCrossentropy()                                 -> it is the classic logistic loss function for binary classification problems
            loss = MeanSquaredError()                                   -> it is the classic linear loss function for regression problems  
            loss = SparseCategoricalCrossentropy()                      -> loss function for multiclass classification problems

        adam optimizer -> an optimizer that improve gradient descent by increasing it if it is too slow or decreasing it if it is too fast
            optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)  -> initialized at 10**-3 = 0,001


        model.fit(x, y, epochs=)                                        -> define how many steps to runs gradient descent on and fits the weights to the data X and y
            epochs = 100                                                -> set the number of steps

        model.summary()                                                 -> provide a summary description of the model

    Step 3 - predict new data and carry out forward propagation
        mode.predict(x_new)

    Regularization on NN

        in the Dense function, add the kernel_regularized parameter and choose lambda
            kernel_regularizer = L2(O.01)

    Dense vs Convolutional nn
        Dense: each neuron takes as input the activation output of all the neurons in the previous layer
        Convolutional: each neuron looks only at a part of the previous layer input.

5) SOFTMAX MULTICLASS CLASSIFICATION 

    Multiclass classification problems = target Y can take on more than 2 possible values 
    - target y still is confined withing a small number of classes, not infinite
    - the output layer returns the probability of y being part of a specific class
    - if there are N classes, there are also N neurons in the output layer
    - each neuron return the probability of y being part of its class

    tensorflow code
        Dense(unit=, activation= 'softmax')
        model.compile(loss=SparseCategoricalCrossentropy())


    More numerically accurate softmax function
        we can get more stable and accurate results if the softmax function and loss function are combined dureing training 

        Dense(unit=, activation= 'linear')                                      ->linear in the output layer instead of softmax
        model.compile((loss=SparseCategoricalCrossentropy(from_logits=True))    -> add from_logits=True This informs the loss function that the softmax operation should be included in the loss calculation.
        model.fit(X,Y, epochs=100)                                              -> same as before
        logits = model(X)                                                       -> now the model return z instead of a
        f_x = tf.nn.softmax(logitz)                                             -> map z back to softmax but now more accurately

    MultiLabel classification -> different from multiclass classification
    multiclass -> returning the probability of an input being part of one of various but definite classes 
                (eg. return which number is present in the image from 0 to 9)
                output results sum up to 1
    multilabel -> return the binary probability that multiple classes are present in the input
                (return if car / bus / pedestrians are present in the image)
                each output is an independent 0 or 1

6) OVERFITTING AND REGULARIZATION

    underfit or high bias                   -> the model does not fit the training set well
    generalization or just right            -> the model fits the training set pretty well
    overfit or high variance                -> the model fits the training set extremely well

    λ is the regularization parameter. 
    usually a small number (for example 1 --> 10)
    if you increase it, it will reduce the size of parameters w and solve the overfitting problem making the model more precise
    if it is too big it will generate and underfit as w will go to 0 and only b will remainder
    if it is too small it won't work 

    Addressing overfit
        1. collect more data
        2. feature selection
        3. regularization = reduce the size of parameters W

        regularization term
            λ/2m * sum(w**2)           -> summ of all the w from 1 to n

        cost function with reg 
            j(w,b) = 1 / 2m * sum (f(x) - y) ** 2 + λ/2m * sum(w**2) 

        gradient with reg 
            derivative w = 1 / m * sum (f(x)-y) * x + λ/m * w 
            derivative b = unchanged since we only apply regularization to w

7) DIAGNOSTICS - BIAS AND VARIANCE - OPTIMIZATION 

        Test that you can run to gain insight into what is or isn't working with a learning algorithm
        goal is to gain guidance into improving its performance
        they take time but they will save you tons of more time (eg spending 6 month to get more training data)

        A) MODEL VALUATION WITH TRAIN AND TEST SET 

            For Regression problems: 
                - split the data set in to 70% train set and 30% test set
                - calculate the cost without regularization for both of them
                - if train set is very low and test set cost is very high, something is going working, else model is good
            
            For classification problems:
                - just calculate how many predictions are wrong: count of y_hat != y
                - if the % is high, something is going wrong, else model is good

        B) MODEL SELECTION AND VALUATION WITH TRAIN, DEV AND TEST SET

            eg how to select the number of layers and neurons in a nn
            eg how to select what order of function to use in regression: classic first order, second order polynomial, third order polynomial ....

            step 1: separate the dataset into 3 set: Train, Dev (or cross-validation), test
                - training	60%	Data used to tune model parameters w and b in training or fitting
                - cross-validation	20%	Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.
                - test	20%	Data used to test the model after tuning to gauge performance on new data
            step 2: run the trian set on different models (eg on 3 models --> generate 3 parameters
            step 3: apply the generated parameters to the dev set 
            step 4: calculate the cost of the 3 models on the dev set and find the lowest one
            step 5: apply the lowest cost model to the test set 
            what we want to select is a model with low j_train and low J_cv

        C) BIAS AND VARIANCE

            High bias = the model underfit the data set
                J_train is high
                J_cv is high 

            High variance = the model overfit the data set 
                J_train is low
                J_cv is high

            We want just right
                J_train is low
                J_train is low

            A -> First we select based on variance and bias by CHoosing the right model (comparing different models) without regularization
            B -> when we choose the right model we then apply regularization to J and we compare different Lambda values starting from 0.01 up to 10 (each time making lambda x2)

            How to know if you have bias or variance?

            1) Compare the baseline level of performance
                - what is the human level of performance?
                - what are competing algrithms level of performance
                - guess based on experience and common sequences

            2) compare the baseline level of performance to the train and dev set
                - eg human 10% - train 10,5% - dev 14% ---> variance
                - eg human 10% - train 15% - dev 15,5% ---> bias   
            
            delta bewteen baseline and train level of performance ---> define if you have a variance problem
            delta between train and dev level of performance ---> define if you have a bias problem 

        D) BIAS AND VARIANCE WITH NEURAL NETOWORKS

            historically -> high bias = too simple of a polynomial model
                         -> high variance = too complex of a polynomial model

            with nn
                -> NN are by nature low bias machine. A bigger NN with good regularization is almost always better than a smaller one
                -> only 1 caveat: bigger NN are computationally more expensive 

                checks
                    -> does it do well on the training set compared to baseline ? NO -> build a bigger NN else move to the next step 
                    -> does it do well on the dev set compared to the train ? NO -> Get more data else move on 

            J_train --> increase and flattens as m grows 
            J_cs --> decrease and flattens as m grows  

            High bias --> more data doesn't help
            High variance --> more data helps!

        E) ERROR ANALYSIS

            - Analyse errors and categorize them into categories based on common traits  
            - try to fix one error group at the time
            - fix first the biggest error groups, those with highest impact 

            Error analysis for skewed data 
                - calculate precision, recall and F1Scrore
                - Precision = true positive / total predicted prositives (true + flase positives)
                - Recall = true positives / total actual positives (true positives + false negatives)
                - F1 Score = 2 * ((P * R) / (P + R))
        
        F) ADDING DATA 

            adding data is almost always helpful. you can
            - add more data on everything
            - add more data on specific data types that where error analysis indicated it might help
            - use data augmentation (modifying an existing training example to create new training examples)
            - use data synthesis (using artificial data inputs to create new training examples)
            - use transfer learning 
                a. download a nn parameters pre-trained on a large data set with same inputs (audio, visual...)
                b. furter train your nn (fine tune) on a smaller data set of your own data 

--- Unsupervised Learning ---

8) CLUSTERING K-MEANS

    Clustering algorithm = look at data and finds groups of data with data points similar to each other

    K-means -> most famous clustering algorithms

    Terminology
        u_k = cluster centroids from 1 to k 
        c_i = index of clusters from i to k to which data point x is assigned
        u_c_i = location of the cluster centroid to which a data point has been assigned


    K-means Algorithm in step 

        step 1: randomely initialize k clusters centroids u_1, ... , u_k
                centroids u have the same dimensions n of the input data 

        repeat 

            step 2: assign points to closest centroid
                    - for i = 1 to m
                    - calculate distance between data point and all centroids (the loss function of the k-means algo)
                    - calculate the min distance / square distance (closest centroid) -> min (x_1 - u_1)**2 -> (the cost function of the k-means algo)
                    - assign data point to closest centroid 

            step 3: relocate centroids to the average positions of their assigned data
                    - calculate the average position of all data points associated to each centroid 
                        - average position is sum(data_points) / count(data_points)
                    - u_k is the position of the centroid and it is now the average position calculate in the previous step

            notes:
                - if a centroids has 0 data points associated to it, eliminate the centroid 

        step 4: apply k-means with difference initializations, 
                calculate each variation's cost function
                choose the best 

    Initializing K-means
        option a: randomely pick k training data point as initialization
                    different initializations can lead to different clustering results 
                    what do you do? run k-means with different initialization and choose the optimal version 
                    (the one with lowest cost function ) 

9) ANOMALY DETECTION 

    Look at a data set and learn to raise a red flag if there is an unusual or anomalous event
        eg fraud detection, machine defect, server control
        good for when you have a small set of possibly very different positive cases and a lot of negative case

    density estimation = probability of x being seen in a dataset
        if probability is lower than our threshold, it is an anomaly
        if probability is higher than our threshold, it is a normal event 

        probability formula = 1 / (squareroot(2 * Pi) * standard_deviation) * exp - ((xj - uj)**2 / (2 * variance))

    Anomaly detection for a single feature (normal distribution)
        the representation of the probability of X is a gaussian / norma distribution (bell curve)
        2 key numbers are
        u = the average of all data points
        standard deviation = the delta between a data point and u_1
        variance = standard deviation ** 2

    Anomaly detection alghorithm x multiple features

        formulas     
            1 to m (i) training examples 
            each example has 1 to n (j) feature (vector)
            P(x_i) = p(x_1) * p(x_2) * .... * p(x_n)
            p(x_j) = u_j = sum(x) / m
            stadard_deviation of x_j = (x_j - u_j)**2 / m

        step by step approach
            1. fit parameters u and standard deviation
            2. given new example, calculate probabilities with formula
            3. compare new example p to threshold epsilon


    Evaluating anomaly detection models and real number evaluation
        Just like in other algorithms
        divide data set into train, cv and test
        data set is labeled y=0 for normal and y=1 for anomaly
        so you actually got labeled data
        - train data set -> train the model on 60% of data points, all normal
        - cv data set -> 20% of data with 50% of anomalies. tune epsilon 
        - test data set -> 20% of data with 50% of anomalies. evaluate the model 

        Find the right epsilon
        - calculate TP, FP, TN, FN, Precision, Recall and F1
        - find the epsilon that maximize F1

    Optimizing features
        - it can happens that features don't have a clean gaussian distribution.
            in this case you should normalize them by applying log or square
        - it can happen that some anomalies are normal looking at a single feature or set of feature, 
            in this case you can add more features or engineer new features to better identify them

10) COLLABORATIVE FILTERING AND CONTENT-BASED RECOMMENDATIONS 

    Predicting recommendations / data points based on your and other people past preferences

    terminology
        n_u = number of users j 
        n_m = number of objects i (eg. movies)
        n = number of features. we can do recommendations with or without features (eg. level of romance, level of action)
        
        r(i,j) = 1 if user j has rated novie m (we have a data point), else 0
        y(i,j) = rating given by user j to movie i (the value of the data point). the F(x)
        x_i = feature vector for object I
        w_j, b_j = parameters for user j

    Recommendations for linear regression problems 
        
        function to learn y(i,j)
            y(i,j) = w_j * x_i + b_j

        cost function to learn w and b for a single user
            1/2 * sum_j((w*j + b) - y)**2 + normalization
            the m is removed at the denominator since it is a constant
            the sum is for all the object of a single users
        
        cost function to learn w and b for all users
            1/2 * sum_nu(sum_j((w*j + b) - y)**2 + normalization
            basically we sum all the cost functions of all users
    
    Collaborative filtering 

        aka you have unknown paramenters x 
        can only be done if we have a lot of users

        first, calculate the cost function to minimize to learn w, b and X
            J(w,b,x) = 1*2 * sum_ij=1(w * x + b - y)**2 + lambda/2 * sum_users(sum_features(w**2)) + lambda/2 * sum_objects(sum_features(x**2))
            function of = sum of all users,object pairs where r = 1 all together
                          regularization term to learn w, summed for all users
                          regularization term to learn x, summed for all objects

        second, calculate gradient descent based on J(2,b,x) for both w, b and X
            w = w - alpha * partial derivative of W wrt to J(w,b,x)
            b = b - alpha * partial derivative of b wrt to J(w,b,x)
            x = x - alpha * partial derivative of x wrt to J(w,b,x)

    collaborative filtering with binary classification

        first you calculate the the new function 
            y(i,j) = g(z) -> where g = 1 / 1 + e**-z and z = w*x+ B

        second you calculate the logistic loss function
            L(f(x)-y) = -y * log(f(x)) - (1-y)log(1-f(x)
        
        third you calculate the cost function 
            J(w,b,x) = 1*2 * sum_ij=1(binary_loss_function)

    Mean normalization with recommendation systems

        eg. imagine a new user without data or some unknown data points related to current users ?

        1. calculate the average of the object related values (rows)
        2. calculate a normalized table of user, object pairs with the new formula f(x) = w * x + b + u where u is the mean 
        3. now all user, object pairs are valid numbers, you can run the recommendation algo much faster and better

    Finding related items (aka product recommendation)

        to evalueate the similarity between objects, calculate the squared distances between their features

        sum_features(x_i - x_k)**2

    Content-based filtering

        collaborative filtering: recommend items to you based on rating of users who gave similar ratings as you
        content-based filtering: recommend items to you based on features of user and items to find good match
        
        X_u j -> user features (age, gender...)
        X_m i -> item features (for e movie example: genre, reviews...)

        V_u j -> the vector of user features
        V_m i -> the vector of item features 

        the function is now F(x) = g(V_u * V_j)  

        how to build the prediction
            a. build a neural network for the user features (user network)
            b. build a neural network for the item features (item network)
            c. calculate the prediction with their outputs V_u and V_m
            d. the cost function is now J = sum(V_u * V_m - y)**2 + NN regularization term
            e. find related items with sum(Vmk - Vmi)**2
        
        If you have a very very large catalogue of items apply Retrieval & Ranking
            Retrieval: generate smaller list of plausible items candidates for the prediction from a large catalogue 
            Ranking: take the list retrieved and build the NN for prediction to the user
                       
11) REINFORCEMENT LEARNING

    ML Algo and models to control the behaviour of object by using reward functions

    Markov Decision Process key elements 
        key philosophy          -> the only things that matter are your current states, the possible actions and the rewards 
        state                   -> the current state you are in (position)
        actions                 -> the possible actions you can take 
        rewards                 -> the rewards assigned to the next milestones 
        discount factor gamma   -> the discount factor used to connect time and rewards. usually 0,9 - 0,99 - 0,999  
        return                  -> the reward-discount function to calculate the final return: R1 + R2*gamma + R3*gamma**2 + R4*gamma**3....unitle milestone
        Policy                  -> the policy used to govern the state - actions - rewards relationship 

    State - action value function q

        it is the function Q(s,a) that
            - determines the Return if you 
            - start from state S
            - and take action a once
            - behave optimally according to policy after that
        
        the goal is to maximize Q(s,a)
            the best possible return in state s is the max of Q(s,a)
            the best possible action a is the action that gives max of Q(s, a)
            the max of Q, called Optimal Q is also defined as Q*

        the goal of a reinforcement learning algo is to calculate all possible Q(s, a) given all possible s and a and
         choose a policy that will tell us which action a to take in state s so as to maximize the expected optimal return Q* 

    Bellman equation

        s -> current state
        a -> current action
        R(s) -> return of the current state
        s'  -> next state 
        a'  -> next actions

        detailed -> Q(s,a) = R1 + R2*gamma + R3*gamma**2 + R4*gamma**3....unitle milestone

        formalized -> Q(s,a) = R(s) + gamma * max Q(s', a') 

    Stochastic equation 

        in many cases you cannot control the object action with complete precision
        there is a misstep probability -> a statistical probability that will make the object go into a different direction
        eg. for a robot: wind, problematic surface, a push...
        it is called a stochastic enviroment

        in a stochastic enviroment we must calculate the expected return (the average return) considering also the misstep probabilities

        Q(s,a) = R(s) + gamma * E(max Q(s', a'))
        where E = expected, average 

    Continuous state spaces

        discrete state spaces: enviroment where the nr of s is limited and a can take you to one of these states
        continuous state spaces: enviroment where the nr of s is infinite and a is more of a direction and velocity (eg an autonomous veichole)

    Deep reinforcement learning

        we have an outer shell reinforcement learning model
        we use an inner deep learning model to calculate Q(s,a) from inputs s and a
            the NN take s and a as input
            it returns Q(s,a) for all possible a 
            then we can easily find the best option

        step 1: initialize nn randomely as guess of Q(s,a)
        step 2: take 10.000 actions
        step 3: optimize parameters and repeat
                option 1: full exploitation -> pick a that maximizes Q(s,a)
                option 2: epsilon greedy policy
                            -> with 0,95 probability pick a that maximizes Q(s,a) (greedy option, exploitation)
                            -> with 1 ---> 0.05 probability pick a randomely (exploration)

    Mini batching and soft updates

        mini batching is actually useful also in supervised learning

        mini batching = when the input is superbig eg 10m, we can train the model on a smaller set of inputs eg 1000
                        the model will be more unprecise but much faster and eventually leading to the same result that we can fine tune by applying all of the inputs


        soft updates = instead of fully applying Q = Qnew to the reinforcement model loop
                        we can apply Q = Q*0.99 + Q*0.01 in order to make smaller and progressive changes  

--- Tensorflow ---

12) SUMMARY AND STEP BY STEP APPROACH --> ALSO ON JUPYTER

    # importing everything necessary

        import tensorflow as tf 
        from tensorflow.keras import layers
        from tensorflow.keras.layers import Dense
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Normalization
        from tensorflow.keras.activations import linear, relu, sigmoid, softmax
        from tensorflow.keras.losses import BinaryCrossentrop, MeanSquaredError, SparseCategoricalCrossentropy

    # normalize data
    # separate data into train, validation and test
    
    # build a model
        model = Sequential ([
                Dense(units=3, activation='', name ='', kernel_regularizer=''),
                Dense(units=3, activation='', name ='', kernel_regularizer=''),
                Dense(units=3, activation='', name ='', kernel_regularizer='') ]) 

    # call the model and store output
        x = tf.ones((3, 3)) -> input values 
        y = model(x) -> store output values

    # train the model     
        model.compile(optimizer = , loss=, metrics= )                              
        model.fit(x, y, batch_size= , epochs=)                                        
        model.summary()                                                 

    # evaluate the model    
        test_scores = model.evaluate(x_test, y_test, verbose=2)
        print("Test loss:", test_scores[0])
        print("Test accuracy:", test_scores[1])

    # predict new data and carry out forward propagation  
        mode.predict(x_new)

    # save and serialize
        model.save("path_to_my_model")
        del model
        
    # Load and Recreate the exact same model purely from the file:
        model = keras.models.load_model("path_to_my_model")

12) TENSORFLOW FUNDAMENTALS


    Step by step pproach:

        Load the data
        Explore the data with analytics
        Preparare, clean and elaborate the data 
        Build, Train, Evaluate the model
        Tune Hyperparameters
        Deploy the model


    import tensorflow as tf

    tensors basics https://www.tensorflow.org/guide/tensor#basics
        type of tensors (constant, variable, ragged, string, sparse)
        tensors rank
        tensors shape
        tensors size
        tensors indexing
        manipulating shapes

        tf.constant(4)                                          -> create tensor
        tf.Tensor(4, shape=(), dtype=int32)                     -> create tensor
        tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)        -> create tensor
        tf.Tensor([[1. 2.], [3. 4.], [5. 6.]], shape=(3, 2))    -> create tensor 
        tf.zeros([3, 2, 4, 5])                                  -> create tensor of zeros

        tf.convert_to_tensor([1,2,3])                           -> convert to tensor
        numpy_matrix.tensor()                                   -> convert to tensor
        np.array(rank_2_tensor)                                 -> convert a tensor into and np.array 
        rank_2_tensor.numpy()                                   -> convert a tensor into and np.array

        print(tf.add(a, b), "\n")                               -> element-wise addition
        print(tf.multiply(a, b), "\n")                          -> element-wise multiplication 
        print(tf.matmul(a, b), "\n")                            -> matrix multiplication
        print(a + b, "\n") # element-wise addition              -> element-wise addition
        print(a * b, "\n") # element-wise multiplication        -> element-wise multiplication
        print(a @ b, "\n") # matrix multiplication              -> matrix multiplication
                                                                
        print(tf.reduce_max(c))                                 -> Find the largest value
        print(tf.math.argmax(c))                                -> Find the index of the largest value
        print(tf.nn.softmax(c))                                 -> Compute the softmax 

        tf.rank(rank_4_tensor)                                  -> return rank
        tf.shape(rank_4_tensor)                                 -> return shape 
        tf.size(rank_4_tensor)                                  -> return size
        Tensor.dtype(tensor)                                    -> return dtype

        tf.reshape(x, [1, 3])                                   -> reshape tensor / object x
        tf.reshape(rank_3_tensor, [-1])                         -> A `-1` passed in the `shape` argument says "Whatever fits".
        Typically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).
        For this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:

        tf.transpose(tensor)                                    -> transpose a tensor

        Indexing with a scalar removes the axis:
            rank_1_tensor[0].numpy()
            rank_1_tensor[1].numpy()
            rank_1_tensor[-1].numpy()

        Indexing with a : slice keeps the axis:
            print("Everything:", rank_1_tensor[:].numpy())
            print("Before 4:", rank_1_tensor[:4].numpy())
            print("From 4 to the end:", rank_1_tensor[4:].numpy())
            print("From 2, before 7:", rank_1_tensor[2:7].numpy())
            print("Every other item:", rank_1_tensor[::2].numpy())
            print("Reversed:", rank_1_tensor[::-1].numpy())

        # multi axis indexing - Get row and column tensors
            print("Second row:", rank_2_tensor[1, :].numpy())
            print("Second column:", rank_2_tensor[:, 1].numpy())
            print("Last row:", rank_2_tensor[-1, :].numpy())
            print("First item in last column:", rank_2_tensor[0, -1].numpy())
            print("Skip the first row:")
            print(rank_2_tensor[1:, :].numpy(), "\n")

13) KERAS

    Import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers

    0. Train, val, test split 


        Build train, val and test sets manually 

            (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

            # Preprocess the data (these are NumPy arrays)
            x_train = x_train.reshape(60000, 784).astype("float32") / 255
            x_test = x_test.reshape(10000, 784).astype("float32") / 255

            y_train = y_train.astype("float32")
            y_test = y_test.astype("float32")

            # Reserve 10,000 samples for validation
            x_val = x_train[-10000:]
            y_val = y_train[-10000:]
            x_train = x_train[:-10000]
            y_train = y_train[:-1000

    
        or automaticallu reserve a val set with validation_split
            
            The argument value represents the fraction of the data to be reserved for validation,
            so it should be set to a number higher than 0 and lower than 1.
            For instance, validation_split=0.2 means "use 20% of the data for validation"
            The way the validation is computed is by taking the last x% samples of the arrays received by the fit() call, before any shuffling.

            Note that you can only use validation_split when training with NumPy data.
            The argument validation_split (generating a holdout set from the training data) is not supported when training from Dataset objects,
             since this feature requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API.

            model = get_compiled_model()
            model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)


    1. Dense and Sequential Model 

        -> Example
            # Define Sequential model with 3 layers
            model = keras.Sequential(
                [
                    layers.Dense(2, activation="relu", name="layer1"),
                    layers.Dense(3, activation="relu", name="layer2"),
                    layers.Dense(4, name="layer3"),
                ]
                )
            # Call model on a test input
            x = tf.ones((3, 3)) -> input values 
            y = model(x) -> store output values


        -> create a Dense model 
            layer_1 = Dense( units = 3, activation = 'linear' / 'sigmoid')


        -> Create a sequential model
            model = keras.Sequential(
                [
                    layers.Dense(2, activation="relu"),
                    layers.Dense(3, activation="relu"),
                    layers.Dense(4),
                ]
                )

        -> model specifications 
                unit = 3                                                    -> specify the number of neurons in the layer
                activation =                                                -> specify the machine learning function to use linear / sigmoid / RELU / softmax
                name =                                                      -> give a name to the layer
                kernel_regularizer =                                        -> to apply regularization with L2(lambda)
          

        -> Call a model on an input tensor
            when you call it on an input, the shape of the parameters/weight is determined from the shape of the input itself
            x = tf.ones((3, 3)) -> input values 
            y = model(x) -> store output values

        -> Specifying the input shape in advance
            keras.Input(shape=(4,))
            or
            layers.Dense(2, activation="relu", input_shape=(4,))
                A simple alternative is to just pass an input_shape argument to your first layer

        -> most common methods and functions
            model.layers            -> access layers
            len(model.layers)       -> return how many layers are in the model
            model.add()             -> add a new layer to the sequential model
            model.pop()             -> pop a layer from the sequential model
            model.summary()         -> return a summary of the model

    2. save and serialize a model

        model.save("path_to_my_model")                      -> save model in a path
        
        del model
        
        model = keras.models.load_model("path_to_my_model") -> # Recreate the exact same model purely from the file:

    3. Model compile

        aka build the model and specify the training configurations
            - optimizer
            - loss
            - metrics

        model.compile(
            optimizer=keras.optimizers.RMSprop(),  # Optimizer         
            loss=keras.losses.SparseCategoricalCrossentropy(),  # Loss function to minimize
            metrics=[keras.metrics.SparseCategoricalAccuracy()], # List of metrics to monitor
            )

        To train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.
        You pass these to the model as arguments to the compile() method:

        The metrics argument should be a list -- your model can have any number of metrics.
        If your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate the contribution of each output to the total loss of the model.
        in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:

            model.compile(
                optimizer="rmsprop",
                loss="sparse_categorical_crossentropy",
                metrics=["sparse_categorical_accuracy"],
            )

        A. Optimizers 
            - SGD() (with or without momentum)
            - RMSprop()
            - Adam()
            - etc.
        
        B. Losses:
            - MeanSquaredError()                -> it is the classic linear loss function for regression problems
            - KLDivergence()
            - CosineSimilarity()
            - BinaryCrossentropy()              -> it is the classic logistic loss function for binary classification problems
            - SparseCategoricalCrossentropy()   -> loss function for multiclass classification problems
            - etc.
        
        C. Metrics:
            - AUC()
            - Precision()
            - Recall()
            - etc.

    4. model fit 
        
        We call fit(), which will train the model by slicing the data into "batches" of size batch_size,
        and repeatedly iterating over the entire dataset for a given number of epochs.

        history = model.fit(
            x_train,
            y_train,
            batch_size=64,
            epochs=2,
            cllbacks=cllbacks
            validation_data=(x_val, y_val),
            validation_split =
            )

    5. Model evaluate

        We evaluate the model on the test data via evaluate():

        results = model.evaluate(x_test, y_test, batch_size=128)
        print("test loss, test acc:", results)

    6. Model predict

        Generate predictions (probabilities -- the output of the last layer) on new data using `predict`
        
        print("Generate predictions for 3 samples")
        predictions = model.predict(x_test[:3])
        print("predictions shape:", predictions.shape)

    7. callbacks and checkpoints

        A. Using callbacks
            
            Callbacks in Keras are objects that are called at different points during training
             (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.). 
             They can be used to implement certain behaviors, such as:

            Doing validation at different points during training (beyond the built-in per-epoch validation)
            Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold
            Changing the learning rate of the model when training seems to be plateauing
            Doing fine-tuning of the top layers when training seems to be plateauing
            Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded
            Etc.
            
            Callbacks can be passed as a list to your call to fit():

                callbacks = [
                    keras.callbacks.EarlyStopping(
                        monitor="val_loss",      -> # Stop training when `val_loss` is no longer improving
                        min_delta=1e-2,          -> # "no longer improving" being defined as "no better than 1e-2 less"
                        patience=2,              -> # "no longer improving" being further defined as "for at least 2 epochs"
                        verbose=1,
                    )
                ]
                model.fit(
                    x_train,
                    y_train,
                    epochs=20,
                    batch_size=64,
                    callbacks=callbacks,
                    validation_split=0.2,
                )


        B. types of callbacks

            ModelCheckpoint: Periodically save the model.
            EarlyStopping: Stop training when training is no longer improving the validation metrics.
            TensorBoard: periodically write model logs that can be visualized in TensorBoard (more details in the section "Visualization").
            CSVLogger: streams loss and metrics data to a CSV file.
            etc.

        C. Checkpoints
    
            When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.
            The easiest way to achieve this is with the ModelCheckpoint callback:

                model = get_compiled_model()

                callbacks = [
                    keras.callbacks.ModelCheckpoint(
                        # Path where to save the model
                        # The two parameters below mean that we will overwrite
                        # the current checkpoint if and only if
                        # the `val_loss` score has improved.
                        # The saved model name will include the current epoch.
                        filepath="mymodel_{epoch}",
                        save_best_only=True,  # Only save a model if `val_loss` has improved.
                        monitor="val_loss",
                        verbose=1,
                    )
                ]
                model.fit(
                    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2
                )

    8. Saving and loading models 


        Full Models 

            # Save
            model = ...  # Get model (Sequential, Functional Model, or Model subclass)
            model.save('path/to/location')

            # Loading the model back:
            from tensorflow import keras
            model = keras.models.load_model('path/to/location')


        Config only 

            get_config() and from_config()
            
            Save
            Calling config = model.get_config() will return a Python dict containing the configuration of the model.

            Load
            The same model can then be reconstructed 
            via Sequential.from_config(config) (for a Sequential model) 
            or Model.from_config(config) (for a Functional API model).
            The same workflow also works for any serializable layer.

                Layer example:
                layer = keras.layers.Dense(3, activation="relu")
                layer_config = layer.get_config()
                new_layer = keras.layers.Dense.from_config(layer_config)

                Sequential model example:
                model = keras.Sequential([keras.Input((32,)), keras.layers.Dense(1)])
                config = model.get_config()
                new_model = keras.Sequential.from_config(config)

        Weights only 

            You can choose to only save & load a model's weights. This can be useful if:
            - You only need the model for inference: in this case you won't need to restart training, 
            so you don't need the compilation information or optimizer state.
            - You are doing transfer learning: in this case you will be training a new model reusing the state of a prior model,
             so you don't need the compilation information of the prior model.

            tf.keras.layers.Layer.get_weights(): Returns a list of numpy arrays.
            tf.keras.layers.Layer.set_weights(): Sets the model weights to the values in the weights argument.

            Examples.

            def create_layer():
                layer = keras.layers.Dense(64, activation="relu", name="dense_2")
                layer.build((None, 784))
                return layer

            layer_1 = create_layer()
            layer_2 = create_layer()

            # Copy weights from layer 1 to layer 2
            layer_2.set_weights(layer_1.get_weights())
